{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Notebook Structure Overview\n",
    "\n",
    "This notebook follows a robust, industry-standard data science workflow for fraud detection. The process ensures data quality, effective feature engineering, balanced modeling, and thorough evaluation before any exploratory analysis or visualization.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A([Environment Setup & Config])\n",
    "    B{{Data Loading & Preprocessing}}\n",
    "    C([Feature Engineering])\n",
    "    D{{Data Balancing & Preparation}}\n",
    "    E([Model Training & Optimization])\n",
    "    F{{Model Evaluation & Analysis}}\n",
    "    G([Exploratory Data Analysis & Visualization])\n",
    "    H([Results Summary & Cleanup])\n",
    "\n",
    "    A --> B\n",
    "    B --> C\n",
    "    C --> D\n",
    "    D --> E\n",
    "    E --> F\n",
    "    F --> G\n",
    "    G --> H\n",
    "\n",
    "    classDef setup fill:#263238,color:#fff,stroke:#263238,stroke-width:2px\n",
    "    classDef data fill:#37474f,color:#fff,stroke:#37474f,stroke-width:2px\n",
    "    classDef feature fill:#424242,color:#fff,stroke:#424242,stroke-width:2px\n",
    "    classDef balance fill:#212121,color:#fff,stroke:#212121,stroke-width:2px\n",
    "    classDef train fill:#1a237e,color:#fff,stroke:#1a237e,stroke-width:2px\n",
    "    classDef eval fill:#004d40,color:#fff,stroke:#004d40,stroke-width:2px\n",
    "    classDef eda fill:#b71c1c,color:#fff,stroke:#b71c1c,stroke-width:2px\n",
    "    classDef summary fill:#263238,color:#fff,stroke:#263238,stroke-width:2px\n",
    "\n",
    "    A:::setup\n",
    "    B:::data\n",
    "    C:::feature\n",
    "    D:::balance\n",
    "    E:::train\n",
    "    F:::eval\n",
    "    G:::eda\n",
    "    H:::summary\n",
    "```\n",
    "\n",
    "**Structure:**\n",
    "- **Environment Setup & Config:** Import libraries, set paths, and define utility functions.\n",
    "- **Data Loading & Preprocessing:** Load raw data, clean, standardize, and handle missing values.\n",
    "- **Feature Engineering:** Create new features to improve model performance.\n",
    "- **Data Balancing & Preparation:** Apply SMOTE, split data, and scale features.\n",
    "- **Model Training & Optimization:** Train models (LightGBM, Autoencoder, Ensemble) and tune hyperparameters.\n",
    "- **Model Evaluation & Analysis:** Assess model performance using metrics, confusion matrix, ROC, and SHAP.\n",
    "- **Exploratory Data Analysis & Visualization:** Visualize distributions, correlations, time patterns, and Benford's Law.\n",
    "- **Results Summary & Cleanup:** Summarize findings and release resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Enviorenmt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional, Union, Any\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, roc_curve, classification_report\n",
    ")\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "# Machine learning\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import shap\n",
    "\n",
    "#set path to project root\n",
    "PROJECT_ROOT=Path(os.getcwd()).parent\n",
    "print(PROJECT_ROOT)\n",
    "\n",
    "sys.path.append(str(PROJECT_ROOT)+'/src')\n",
    "#change to project root\n",
    "os.chdir(PROJECT_ROOT)\n",
    "from src.utils.optimization_utils import OptimUtils\n",
    "\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.objects as so\n",
    "from seaborn import (\n",
    "    axes_style, set_style, set_theme, set_palette,\n",
    "    heatmap, histplot, barplot, lineplot, scatterplot\n",
    ")\n",
    "from matplotlib.dates import DateFormatter, HourLocator\n",
    "set_theme(style=\"whitegrid\", font_scale=1.2)  # Modern seaborn theme\n",
    "set_palette(\"deep\")  # Modern color palette\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'lines.linewidth': 2\n",
    "})\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Data Loading & Preprocessing\n",
    "\n",
    "Loading and preprocessing the fraud detection datasets:\n",
    "1. Load raw data\n",
    "2. Clean and standardize\n",
    "3. Handle missing values\n",
    "4. Convert data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data using improved OptimUtils\n",
    "banking_path = f\"{PROJECT_ROOT}/data/input/ieee-fraud/ieee-fraud.csv\"\n",
    "credit_path = f\"{PROJECT_ROOT}/data/input/creditcard-fraud/creditcard.csv\"\n",
    "\n",
    "# Load data directly with optimized settings\n",
    "ieee_data = OptimUtils.process_dataframe(banking_path).compute()\n",
    "credit_data = OptimUtils.process_dataframe(credit_path).compute()\n",
    "\n",
    "print(f\"\\nLoaded {len(ieee_data):,} banking and {len(credit_data):,} credit card transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "\n",
    "\n",
    "def preprocess_dataset(df, dataset_type: str):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing for fraud detection datasets\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        dataset_type: 'banking' or 'creditcard'\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Standardize column names\n",
    "    df.columns = [re.sub(r'[^a-zA-Z0-9_]', '_', col.lower().replace(' ', '_')) for col in df.columns]\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    removed_duplicates = initial_rows - len(df)\n",
    "    if removed_duplicates > 0:\n",
    "        print(f\"Removed {removed_duplicates} duplicate rows\")\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['object', 'category']:\n",
    "                df[col] = df[col].fillna('unknown')\n",
    "            else:\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # 4. Dataset-specific processing\n",
    "    if dataset_type == 'banking':\n",
    "        df = _preprocess_banking(df)\n",
    "    elif dataset_type == 'creditcard':\n",
    "        df = _preprocess_creditcard(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _preprocess_banking(df):\n",
    "    \"\"\"Preprocess banking dataset\"\"\"\n",
    "    \n",
    "    # Convert datetime\n",
    "    df['transaction_time'] = pd.to_datetime(df['transaction_time'])\n",
    "    df['hour'] = df['transaction_time'].dt.hour\n",
    "    df['day'] = df['transaction_time'].dt.dayofweek\n",
    "    df['is_weekend'] = ((df['day'] >= 5).astype(int))\n",
    "    \n",
    "    # Handle transaction amount outliers\n",
    "\n",
    "    q1, q3 = df['transaction_amount'].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    # Cap outliers instead of removing\n",
    "    df['transaction_amount_capped'] = df['transaction_amount'].clip(lower=lower_bound, upper=upper_bound)\n",
    "    df['transaction_amount_log'] = np.log1p(df['transaction_amount_capped'])\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = ['transaction_type', 'transaction_location', 'device_used']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "    # Create transaction type indicators\n",
    "    df['is_deposit'] = (df['transaction_type'] == 'deposit').astype(int)\n",
    "    df['is_withdrawal'] = (df['transaction_type'] == 'withdrawal').astype(int)\n",
    "    df['is_transfer'] = (df['transaction_type'] == 'transfer').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def _preprocess_creditcard(df):\n",
    "    \"\"\"Preprocess credit card dataset\"\"\"\n",
    "    \n",
    "    # V columns are already PCA transformed - no need to standardize\n",
    "    # Just verify they're properly scaled\n",
    "    v_cols = [col for col in df.columns if col.startswith('v')]\n",
    "    if v_cols:\n",
    "        # Check if V columns are already standardized\n",
    "        v_means = df[v_cols].mean().abs().max()\n",
    "        v_stds = df[v_cols].std().abs().max()\n",
    "        \n",
    "        if v_means > 0.1 or v_stds > 2.0:\n",
    "            print(\"Warning: V columns may not be properly standardized\")\n",
    "    \n",
    "    # Handle amount outliers - use lowercase column name after standardization\n",
    "    q1, q3 = df['amount'].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    \n",
    "    df['amount_capped'] = df['amount'].clip(lower=lower_bound, upper=upper_bound)\n",
    "    df['amount_log'] = np.log1p(df['amount_capped'])\n",
    "    \n",
    "    # Convert time to datetime features\n",
    "   \n",
    "    df['hour'] = (df['time'] / 3600).astype(int) % 24\n",
    "    df['day'] = (df['time'] / 86400).astype(int) % 7\n",
    "    df['is_weekend'] = ((df['day'] >= 5).astype(int))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print column names of the raw data to keep track of the columns\n",
    "raw_data_cols_bnk=ieee_data.columns.tolist()\n",
    "raw_data_cols_cc=credit_data.columns.tolist()\n",
    "\n",
    "print(f'{raw_data_cols_bnk}')\n",
    "print(f'{raw_data_cols_cc}')\n",
    "\n",
    "#clean and preprocess the data and print the shapes of the datasets\n",
    "banking_df = preprocess_dataset(ieee_data, 'banking')\n",
    "credit_df = preprocess_dataset(credit_data, 'creditcard')\n",
    "print (f\"After Data Cleaning and Preprocessing:\\nBanking df: {banking_df.columns.tolist()}.\\nCredit df: {credit_df.columns.tolist()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Analyzing the characteristics and distributions of our datasets:\n",
    "1. Class Distribution Analysis\n",
    "2. Feature Distributions\n",
    "3. Correlation Analysis\n",
    "4. Transaction Amount Analysis\n",
    "5. Time-based Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(df: pd.DataFrame, hour_col: str, fraud_col: str, title: str) -> None:\n",
    "    \"\"\"Analyzes transaction timing patterns\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Hourly patterns\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fraud_hourly = df[df[fraud_col] == 1][hour_col].value_counts(normalize=True).sort_index()\n",
    "    normal_hourly = df[df[fraud_col] == 0][hour_col].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    plt.plot(fraud_hourly.index, fraud_hourly.values, label='Fraudulent', color='red', marker='o')\n",
    "    plt.plot(normal_hourly.index, normal_hourly.values, label='Normal', color='blue', marker='o')\n",
    "    plt.title(f\"{title}\\nHourly Transaction Patterns\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Transaction Frequency\")\n",
    "    plt.legend(frameon=True, facecolor='white', framealpha=0.9)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Fraud rate by hour\n",
    "    plt.subplot(1, 2, 2)\n",
    "    hourly_fraud_rate = df.groupby(hour_col)[fraud_col].mean() * 100\n",
    "    \n",
    "    # Use a line plot instead of bars to avoid overlapping\n",
    "    plt.plot(hourly_fraud_rate.index, hourly_fraud_rate.values, \n",
    "             marker='o', linestyle='-', color='purple', linewidth=2)\n",
    "    plt.title(f\"{title}\\nFraud Rate by Hour\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Fraud Rate (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adjust y-axis\n",
    "    max_rate = hourly_fraud_rate.max() \n",
    "    plt.ylim(0, max_rate * 1.2)  # Add 20% padding above the highest point\n",
    "    \n",
    "    # Label only top 6 fraud rates\n",
    "    top_indices = hourly_fraud_rate.nlargest(6).index\n",
    "    for i in top_indices:\n",
    "        rate = hourly_fraud_rate[i]\n",
    "        plt.text(i, rate + (max_rate * 0.05), f'{rate:.1f}%', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Credit Card Analysis\n",
    "print(\"Credit Card Fraud Analysis\")\n",
    "print(\"-\" * 50)\n",
    "analyze_temporal_patterns(credit_df, 'hour', 'class', \"Credit Card Transactions\")\n",
    "\n",
    "# Banking Analysis\n",
    "print(\"\\nBanking Fraud Analysis\")\n",
    "print(\"-\" * 50)\n",
    "analyze_temporal_patterns(banking_df, 'hour', 'is_fraud', \"Banking Transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Creating additional features to improve model performance:\n",
    "1. Temporal features\n",
    "2. Amount-based features\n",
    "3. Statistical aggregations\n",
    "4. Interaction features\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Benford's Law Analysis\n",
    "\n",
    "Analyzing transaction amounts using Benford's Law:\n",
    "1. First digit distribution\n",
    "2. Fraud vs legitimate patterns\n",
    "3. Deviation metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create additional features that can help detect fraud:\n",
    "1. Temporal features (hour, day, weekend)\n",
    "2. Transaction pattern features\n",
    "3. Amount-based features\n",
    "4. Statistical aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features_partition(df, dataset_name):\n",
    "    \"\"\"Direct feature engineering for both datasets\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Banking dataset features\n",
    "    if dataset_name == 'Banking':\n",
    "        df['amount_log'] = np.log1p(df['transaction_amount'])\n",
    "        df['is_round_amount'] = (df['transaction_amount'] % 100 == 0).astype(int)\n",
    "        df['type_encoded'] = df['transaction_type'].astype('category').cat.codes\n",
    "        df['transaction_text'] = df.apply(\n",
    "            lambda row: f\"Transaction type: {row['transaction_type']}, Amount: ${row['transaction_amount']:.2f}, Device: {row['device_used']}, Time: {row['hour']}:00\", \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "    # Credit Card dataset features\n",
    "    elif dataset_name == 'Credit Card':\n",
    "        df['amount_log'] = np.log1p(df['amount'])\n",
    "        df['is_round_amount'] = (df['amount'] % 10 == 0).astype(int)\n",
    " \n",
    "\n",
    "    return df\n",
    "\n",
    "# Process both datasets\n",
    "banking_engineered = engineer_features_partition(banking_df, 'Banking')\n",
    "credit_engineered = engineer_features_partition(credit_df, 'Credit Card')\n",
    "print(banking_engineered.head())\n",
    "print(credit_engineered.head())   \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 4. Data Balancing with SMOTE\n",
    "\n",
    "Since fraud is a rare event, we'll use SMOTE to balance the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns() -> List[str]:\n",
    "    \"\"\"Return list of features used for training.\"\"\"\n",
    "    return [\n",
    "        'amount', 'amount_log', 'hour', 'is_weekend', 'is_night',\n",
    "        'is_business_hours', 'is_round_amount', 'amount_hour_percentile',\n",
    "        'amount_deviation', 'trans_freq_hour'\n",
    "    ]\n",
    "\n",
    "def split_and_scale_data(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data and scale features.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    return (scaler.fit_transform(X_train), \n",
    "            scaler.transform(X_test),\n",
    "            y_train, y_test)\n",
    "\n",
    "def balance_with_smote(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply SMOTE to balance the dataset.\"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    return smote.fit_resample(X, y)\n",
    "\n",
    "def prepare_training_data(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for model training using functional approach.\"\"\"\n",
    "    feature_cols = get_feature_columns()\n",
    "    X, y = df[feature_cols].values, df['fraud'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_and_scale_data(X, y)\n",
    "    X_train_balanced, y_train_balanced = balance_with_smote(X_train, y_train)\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "# Prepare datasets\n",
    "training_data = {\n",
    "    'banking': prepare_training_data(banking_featured),\n",
    "    'credit': prepare_training_data(credit_featured)\n",
    "}\n",
    "\n",
    "# Create data summary\n",
    "data_summary = pd.DataFrame({\n",
    "    'Dataset': ['Banking', 'Credit'],\n",
    "    'Training Samples': [training_data['banking'][0].shape[0], \n",
    "                        training_data['credit'][0].shape[0]],\n",
    "    'Testing Samples': [training_data['banking'][1].shape[0], \n",
    "                       training_data['credit'][1].shape[0]],\n",
    "    'Features': [training_data['banking'][0].shape[1], \n",
    "                training_data['credit'][0].shape[1]],\n",
    "    'Balanced Ratio': [np.mean(training_data['banking'][2]), \n",
    "                      np.mean(training_data['credit'][2])]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 5. Model Architecture & Training\n",
    "\n",
    "Model training pipeline:\n",
    "1. Dimensionality Reduction (PCA)\n",
    "2. Gradient Boosting with Feature Selection\n",
    "3. Model Ensemble:\n",
    "   - Random Forest\n",
    "   - LightGBM\n",
    "   - Neural Network\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Model Training & Evaluation\n",
    "\n",
    "Training and evaluating fraud detection models:\n",
    "1. Data preparation with SMOTE\n",
    "2. LightGBM with Optuna optimization\n",
    "3. Model evaluation\n",
    "4. SHAP analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = 'fraud',\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare train/test splits with SMOTE\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Target column name\n",
    "        test_size: Test set proportion\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test arrays\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    X = df.drop([target_col, 'timestamp'], axis=1, errors='ignore')\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split first to avoid data leakage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "def optimize_lightgbm(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    n_trials: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        n_trials: Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "        Best parameters\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True)\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = lgb.cv(\n",
    "            param,\n",
    "            lgb.Dataset(X_train, y_train),\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=50,\n",
    "            stratified=True,\n",
    "            nfold=5\n",
    "        )\n",
    "        \n",
    "        return cv_scores['auc-mean'][-1]\n",
    "    \n",
    "    # Run optimization\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "def train_and_evaluate(\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    title: str\n",
    ") -> lgb.Booster:\n",
    "    \"\"\"\n",
    "    Train LightGBM model and evaluate performance\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        y_train: Training labels\n",
    "        y_test: Test labels\n",
    "        title: Dataset title\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Optimize hyperparameters\n",
    "    print(f\"\\nOptimizing LightGBM for {title}...\")\n",
    "    best_params = optimize_lightgbm(X_train, y_train)\n",
    "    \n",
    "    # Train model\n",
    "    train_data = lgb.Dataset(X_train, y_train)\n",
    "    model = lgb.train({**best_params, 'objective': 'binary'}, train_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\nResults for {title}:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "    \n",
    "    print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {title}')\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def analyze_shap(\n",
    "    model: lgb.Booster,\n",
    "    X_test: np.ndarray,\n",
    "    feature_names: List[str],\n",
    "    title: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Analyze feature importance using SHAP\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        feature_names: Feature names\n",
    "        title: Dataset title\n",
    "    \"\"\"\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names)\n",
    "    plt.title(f'SHAP Feature Importance - {title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, X_test, feature_names=feature_names, plot_type='bar')\n",
    "    plt.title(f'SHAP Feature Importance (Bar) - {title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate banking model\n",
    "print(\"Preparing banking data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_train_test_data(banking_featured)\n",
    "bank_model = train_and_evaluate(X_train_bank, X_test_bank, y_train_bank, y_test_bank, \"Banking\")\n",
    "\n",
    "print(\"\\nAnalyzing banking model with SHAP...\")\n",
    "analyze_shap(bank_model, X_test_bank, banking_featured.drop(['fraud', 'timestamp'], axis=1, errors='ignore').columns, \"Banking\")\n",
    "\n",
    "# Train and evaluate credit card model\n",
    "print(\"\\nPreparing credit card data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_train_test_data(credit_featured)\n",
    "credit_model = train_and_evaluate(X_train_credit, X_test_credit, y_train_credit, y_test_credit, \"Credit Card\")\n",
    "\n",
    "print(\"\\nAnalyzing credit card model with SHAP...\")\n",
    "analyze_shap(credit_model, X_test_credit, credit_featured.drop(['fraud', 'timestamp'], axis=1, errors='ignore').columns, \"Credit Card\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = 'fraud',\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare train/test splits with proper SMOTE application\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of test set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test arrays\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split first to avoid data leakage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "    \n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    return X_train_balanced, X_test_scaled, y_train_balanced, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(\n",
    "    model,\n",
    "    X_train: np.ndarray,\n",
    "    feature_names: List[str],\n",
    "    max_display: int = 20\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Analyze feature importance using SHAP values\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model (must be compatible with SHAP)\n",
    "        X_train: Training data\n",
    "        feature_names: List of feature names\n",
    "        max_display: Maximum number of features to display\n",
    "    \"\"\"\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    # For binary classification, shap_values is a list with [negative_class, positive_class]\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Use positive class (fraud) values\n",
    "    \n",
    "    # Create summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_train,\n",
    "        feature_names=feature_names,\n",
    "        max_display=max_display,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title('SHAP Feature Importance Analysis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create bar plot of mean absolute SHAP values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_train,\n",
    "        feature_names=feature_names,\n",
    "        max_display=max_display,\n",
    "        plot_type='bar',\n",
    "        show=False\n",
    "    )\n",
    "    plt.title('Mean Impact on Model Output Magnitude')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 4. Pattern Analysis and Visualization\n",
    "\n",
    "We'll analyze and visualize:\n",
    "1. Temporal patterns in fraudulent transactions\n",
    "2. Amount distribution analysis\n",
    "3. Feature importance across models\n",
    "4. Anomaly detection thresholds\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Comprehensive Fraud Detection System\n",
    "\n",
    "This notebook implements a multi-model fraud detection system using:\n",
    "- LightGBM with Optuna optimization\n",
    "- Neural Network (Autoencoder)\n",
    "- FinBERT for text analysis\n",
    "- SHAP for model interpretability\n",
    "\n",
    "## Structure\n",
    "\n",
    "1. Environment Setup & Configuration\n",
    "   - Library Imports\n",
    "   - DASK Setup\n",
    "   - Random Seeds\n",
    "   - Visualization Settings\n",
    "\n",
    "2. Data Loading & Quality Checks\n",
    "   - Path Configuration\n",
    "   - Data Loading with DASK\n",
    "   - Benford's Law Analysis\n",
    "   - Quality Assessment\n",
    "   - Statistical Pattern Analysis\n",
    "\n",
    "3. Data Preprocessing\n",
    "   - Banking Data Processing\n",
    "   - Credit Card Data Processing\n",
    "   - Text Feature Processing (FinBERT)\n",
    "   - Feature Engineering\n",
    "\n",
    "4. Model Training\n",
    "   - Data Splitting (with proper SMOTE)\n",
    "   - LightGBM with Optuna\n",
    "   - Autoencoder Implementation\n",
    "   - FinBERT Fine-tuning\n",
    "   \n",
    "5. Evaluation & Analysis\n",
    "   - Performance Metrics\n",
    "   - SHAP Analysis\n",
    "   - Feature Importance\n",
    "   - Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load transaction datasets\n",
    "print(\"Loading transaction data...\")\n",
    "banking_files = pd.read_csv('data/input/banking-fraud/*.csv').compute()\n",
    "credit_files = pd.read_csv('data/input/creditcard-fraud/*.csv').compute()\n",
    "\n",
    "\n",
    "# Initialize fraud detection system\n",
    "print(\"Initializing fraud detection system...\")\n",
    "fraud_detector = MultiModalFraudDetector()\n",
    "\n",
    "# Prepare training data\n",
    "print(\"Preparing training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ieee_data.drop('is_fraud', axis=1),\n",
    "    ieee_data['is_fraud'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the system\n",
    "print(\"Training fraud detection components...\")\n",
    "fraud_detector.train_system(X_train, y_train)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Evaluating system performance...\")\n",
    "test_predictions = [\n",
    "    fraud_detector.analyze_transaction(transaction.reshape(1, -1)) > 0.5 \n",
    "    for transaction in X_test.values\n",
    "]\n",
    "\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(classification_report(y_test, test_predictions))\n",
    "\n",
    "auc_score = roc_auc_score(y_test, test_predictions)\n",
    "print(f\"\\nAUC-ROC Score: {auc_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=20, title_prefix=\"\"):\n",
    "    \"\"\"Plot feature importance with enhanced styling\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[-top_n:]\n",
    "    \n",
    "    plt.title(f'{title_prefix}Top {top_n} Feature Importances', pad=20)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    bars = plt.barh(range(len(indices)), importance[indices])\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    return plt.gca()\n",
    "\n",
    "\n",
    "def plot_fraud_distribution(df, amount_col='amount', fraud_col='fraud', title_prefix=\"\"):\n",
    "    \"\"\"Plot fraud amount distribution with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create two subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=fraud_col, y=amount_col, data=df)\n",
    "    plt.title(f'{title_prefix}Transaction Amount Distribution by Class', pad=20)\n",
    "    plt.xlabel('Fraud (1) vs Normal (0)')\n",
    "    plt.ylabel('Amount')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data=df, x=amount_col, hue=fraud_col, multiple=\"stack\", bins=50)\n",
    "    plt.title(f'{title_prefix}Transaction Amount Histogram by Class', pad=20)\n",
    "    plt.xlabel('Amount')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def plot_time_patterns(df, hour_col='hour', fraud_col='fraud', title_prefix=\"\"):\n",
    "    \"\"\"Plot time-based patterns with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create two subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fraud_by_hour = df.groupby([hour_col, fraud_col]).size().unstack()\n",
    "    fraud_by_hour.plot(kind='line', marker='o')\n",
    "    plt.title(f'{title_prefix}Transaction Patterns by Hour', pad=20)\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    plt.legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if 'day_of_week' in df.columns:\n",
    "        fraud_by_day = df.groupby(['day_of_week', fraud_col]).size().unstack()\n",
    "        fraud_by_day.plot(kind='bar')\n",
    "        plt.title(f'{title_prefix}Transaction Patterns by Day', pad=20)\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with Advanced Techniques\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Splitting with SMOTE (After Split)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def prepare_train_test_data(\n",
    "    df: dd.DataFrame,\n",
    "    target_col: str = 'fraud',\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare train/test splits with proper SMOTE application\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of test set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Convert to numpy for sklearn\n",
    "    df = df.compute()\n",
    "    X = df.drop([target_col], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split first\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# LightGBM with Optuna Optimization\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def optimize_lightgbm(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_valid: np.ndarray,\n",
    "    y_valid: np.ndarray,\n",
    "    n_trials: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_valid: Validation features\n",
    "        y_valid: Validation labels\n",
    "        n_trials: Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "        Best parameters\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "            'min_split_gain': trial.suggest_loguniform('min_split_gain', 1e-8, 1.0),\n",
    "            'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-8, 10.0)\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"\\nBest AUC: {study.best_value:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Autoencoder for Anomaly Detection\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class FraudAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, encoding_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder with batch normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.BatchNorm1d(encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder with batch normalization\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(\n",
    "    X_train: np.ndarray,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> Tuple[FraudAutoencoder, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train autoencoder and get reconstruction error scores\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and reconstruction errors\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FraudAutoencoder(input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create data loader\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(x)\n",
    "            loss = criterion(reconstructed, x)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(loader):.6f}\")\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_tensor)\n",
    "        errors = torch.mean(torch.pow(X_tensor - reconstructed, 2), dim=1)\n",
    "        reconstruction_errors = errors.cpu().numpy()\n",
    "    \n",
    "    return model, reconstruction_errors\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing banking fraud data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_train_test_data(\n",
    "    banking_processed\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_train_test_data(\n",
    "    credit_processed\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing IEEE-CIS fraud data...\")\n",
    "X_train_ieee, X_test_ieee, y_train_ieee, y_test_ieee = prepare_train_test_data(\n",
    "    ieee_processed\n",
    ")\n",
    "\n",
    "# Train autoencoders\n",
    "print(\"\\nTraining autoencoder for banking fraud...\")\n",
    "autoencoder_bank, errors_bank = train_autoencoder(X_train_bank)\n",
    "\n",
    "print(\"\\nTraining autoencoder for credit card fraud...\")\n",
    "autoencoder_credit, errors_credit = train_autoencoder(X_train_credit)\n",
    "\n",
    "print(\"\\nTraining autoencoder for IEEE fraud...\")\n",
    "autoencoder_ieee, errors_ieee = train_autoencoder(X_train_ieee)\n",
    "\n",
    "# Add reconstruction error as a feature\n",
    "X_train_bank = np.column_stack([X_train_bank, errors_bank])\n",
    "X_test_bank = np.column_stack([\n",
    "    X_test_bank,\n",
    "    train_autoencoder(X_test_bank, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_credit = np.column_stack([X_train_credit, errors_credit])\n",
    "X_test_credit = np.column_stack([\n",
    "    X_test_credit,\n",
    "    train_autoencoder(X_test_credit, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_ieee = np.column_stack([X_train_ieee, errors_ieee])\n",
    "X_test_ieee = np.column_stack([\n",
    "    X_test_ieee,\n",
    "    train_autoencoder(X_test_ieee, epochs=1)[1]\n",
    "])\n",
    "\n",
    "# Optimize and train LightGBM models\n",
    "print(\"\\nOptimizing LightGBM for banking fraud...\")\n",
    "lgb_params_bank = optimize_lightgbm(\n",
    "    X_train_bank, y_train_bank,\n",
    "    X_test_bank, y_test_bank\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for credit card fraud...\")\n",
    "lgb_params_credit = optimize_lightgbm(\n",
    "    X_train_credit, y_train_credit,\n",
    "    X_test_credit, y_test_credit\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for IEEE fraud...\")\n",
    "lgb_params_ieee = optimize_lightgbm(\n",
    "    X_train_ieee, y_train_ieee,\n",
    "    X_test_ieee, y_test_ieee\n",
    ")\n",
    "\n",
    "# Train final models with best parameters\n",
    "lgb_bank = lgb.LGBMClassifier(**lgb_params_bank)\n",
    "lgb_bank.fit(X_train_bank, y_train_bank)\n",
    "\n",
    "lgb_credit = lgb.LGBMClassifier(**lgb_params_credit)\n",
    "lgb_credit.fit(X_train_credit, y_train_credit)\n",
    "\n",
    "lgb_ieee = lgb.LGBMClassifier(**lgb_params_ieee)\n",
    "lgb_ieee.fit(X_train_ieee, y_train_ieee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Visualization Functions\n",
    "\n",
    "@plot_with_style\n",
    "def plot_roc_curve(y_true, y_pred_proba, title_prefix=\"\"):\n",
    "    \"\"\"Plot ROC curve with enhanced styling\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix}Receiver Operating Characteristic (ROC) Curve', pad=20)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, facecolor='white', framealpha=1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_confusion_matrix(y_true, y_pred, title_prefix=\"\"):\n",
    "    \"\"\"Plot confusion matrix with enhanced styling\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with both count and percentage\n",
    "    annot = np.empty_like(cm, dtype=str)\n",
    "    np.fill_diagonal(annot, [f'{val}\\n({p:.1f}%)' for val, p in zip(np.diag(cm), np.diag(cm_percent))])\n",
    "    mask = ~np.eye(cm.shape[0], dtype=bool)\n",
    "    annot[mask] = [f'{val}\\n({p:.1f}%)' for val, p in zip(cm[mask], cm_percent[mask])]\n",
    "    \n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', cbar=True,\n",
    "                xticklabels=['Not Fraud', 'Fraud'],\n",
    "                yticklabels=['Not Fraud', 'Fraud'])\n",
    "    \n",
    "    plt.title(f'{title_prefix}Confusion Matrix', pad=20)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    return plt.gca()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Analysis\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Performance Metrics\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_score, avg_precision\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# SHAP Analysis\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def analyze_shap_values(\n",
    "    model: Any,\n",
    "    X_test: np.ndarray,\n",
    "    feature_names: List[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using SHAP values\n",
    "    \"\"\"\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # For binary classification\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Importance - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed plot for top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        max_display=10,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Summary Plot - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate banking fraud model\n",
    "print(\"Evaluating banking fraud model...\")\n",
    "banking_auc, banking_ap = evaluate_model(\n",
    "    lgb_bank,\n",
    "    X_test_bank,\n",
    "    y_test_bank,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "# Evaluate credit card fraud model\n",
    "print(\"\\nEvaluating credit card fraud model...\")\n",
    "credit_auc, credit_ap = evaluate_model(\n",
    "    lgb_credit,\n",
    "    X_test_credit,\n",
    "    y_test_credit,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n",
    "\n",
    "# Evaluate IEEE fraud model\n",
    "print(\"\\nEvaluating IEEE fraud model...\")\n",
    "ieee_auc, ieee_ap = evaluate_model(\n",
    "    lgb_ieee,\n",
    "    X_test_ieee,\n",
    "    y_test_ieee,\n",
    "    \"IEEE-CIS Fraud\"\n",
    ")\n",
    "\n",
    "# SHAP analysis for each model\n",
    "print(\"\\nAnalyzing feature importance with SHAP...\")\n",
    "\n",
    "# Banking model\n",
    "analyze_shap_values(\n",
    "    lgb_bank,\n",
    "    X_test_bank,\n",
    "    banking_processed.drop('fraud', axis=1).columns,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "# Credit card model\n",
    "analyze_shap_values(\n",
    "    lgb_credit,\n",
    "    X_test_credit,\n",
    "    credit_processed.drop('fraud', axis=1).columns,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n",
    "\n",
    "# IEEE model\n",
    "analyze_shap_values(\n",
    "    lgb_ieee,\n",
    "    X_test_ieee,\n",
    "    ieee_processed.drop('fraud', axis=1).columns,\n",
    "    \"IEEE-CIS Fraud\"\n",
    ")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Banking Fraud AUC: {banking_auc:.4f}\")\n",
    "print(f\"Credit Card Fraud AUC: {credit_auc:.4f}\")\n",
    "print(f\"IEEE-CIS Fraud AUC: {ieee_auc:.4f}\")\n",
    "print(f\"\\nMean AUC across datasets: {np.mean([banking_auc, credit_auc, ieee_auc]):.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Training\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Splitting with Proper SMOTE\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def prepare_train_test_data(\n",
    "    df: dd.DataFrame,\n",
    "    target_col: str,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare train/test splits with proper SMOTE application\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of test set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Convert to numpy for sklearn\n",
    "    df = df.compute()\n",
    "    X = df.drop(target_col, axis=1).values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Split first\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"Class distribution before SMOTE:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing banking fraud data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_train_test_data(\n",
    "    banking_processed, 'fraud'\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_train_test_data(\n",
    "    credit_processed, 'Class'\n",
    ")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# LightGBM with Optuna Optimization\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def optimize_lightgbm(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_valid: np.ndarray,\n",
    "    y_valid: np.ndarray,\n",
    "    n_trials: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_valid: Validation features\n",
    "        y_valid: Validation labels\n",
    "        n_trials: Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "        Best parameters\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0)\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# Optimize and train LightGBM models\n",
    "print(\"Optimizing LightGBM for banking fraud...\")\n",
    "lgb_params_bank = optimize_lightgbm(\n",
    "    X_train_bank, y_train_bank,\n",
    "    X_test_bank, y_test_bank\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for credit card fraud...\")\n",
    "lgb_params_credit = optimize_lightgbm(\n",
    "    X_train_credit, y_train_credit,\n",
    "    X_test_credit, y_test_credit\n",
    ")\n",
    "\n",
    "# Train final models with best parameters\n",
    "lgb_bank = lgb.LGBMClassifier(**lgb_params_bank)\n",
    "lgb_bank.fit(X_train_bank, y_train_bank)\n",
    "\n",
    "lgb_credit = lgb.LGBMClassifier(**lgb_params_credit)\n",
    "lgb_credit.fit(X_train_credit, y_train_credit)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Autoencoder Implementation\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class FraudAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, encoding_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(\n",
    "    X_train: np.ndarray,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> Tuple[FraudAutoencoder, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train autoencoder and get reconstruction error scores\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and reconstruction errors\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FraudAutoencoder(input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create data loader\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(x)\n",
    "            loss = criterion(reconstructed, x)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(loader):.6f}\")\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_tensor)\n",
    "        errors = torch.mean(torch.pow(X_tensor - reconstructed, 2), dim=1)\n",
    "        reconstruction_errors = errors.cpu().numpy()\n",
    "    \n",
    "    return model, reconstruction_errors\n",
    "\n",
    "# Train autoencoders\n",
    "print(\"\\nTraining autoencoder for banking fraud...\")\n",
    "autoencoder_bank, errors_bank = train_autoencoder(X_train_bank)\n",
    "\n",
    "print(\"\\nTraining autoencoder for credit card fraud...\")\n",
    "autoencoder_credit, errors_credit = train_autoencoder(X_train_credit)\n",
    "\n",
    "# Add reconstruction error as a feature\n",
    "X_train_bank = np.column_stack([X_train_bank, errors_bank])\n",
    "X_test_bank = np.column_stack([\n",
    "    X_test_bank,\n",
    "    train_autoencoder(X_test_bank, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_credit = np.column_stack([X_train_credit, errors_credit])\n",
    "X_test_credit = np.column_stack([\n",
    "    X_test_credit,\n",
    "    train_autoencoder(X_test_credit, epochs=1)[1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Training Setup\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Splitting Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def prepare_model_data(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare data for model training including train-test split.\n",
    "    \n",
    "    Args:\n",
    "        df: Processed DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of data to use for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test arrays\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data\n",
    "    return train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Model Configuration\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Random Forest parameters\n",
    "rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# LightGBM parameters\n",
    "lgb_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Prepare Training Data\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Preparing banking fraud data...\")\n",
    "banking_X_train, banking_X_test, banking_y_train, banking_y_test = prepare_model_data(\n",
    "    banking_processed, 'fraud'\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "credit_X_train, credit_X_test, credit_y_train, credit_y_test = prepare_model_data(\n",
    "    credit_processed, 'Class'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Data Shapes:\")\n",
    "print(f\"Banking: {banking_X_train.shape}, {banking_y_train.shape}\")\n",
    "print(f\"Credit Card: {credit_X_train.shape}, {credit_y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_credit_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing for credit card fraud dataset\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    print(\"\\nPreprocessing Credit Card Fraud Dataset:\")\n",
    "    \n",
    "    # Handle missing values in V1-V28 and Amount\n",
    "    v_columns = [col for col in df.columns if col.startswith('V')]\n",
    "    num_columns = v_columns + ['Amount']\n",
    "    \n",
    "    for col in num_columns:\n",
    "        if df[col].isnull().any():\n",
    "            fill_value = df[col].mean()\n",
    "            df[col].fillna(fill_value, inplace=True)\n",
    "            print(f\"Filled nulls in {col} with mean: {fill_value:.2f}\")\n",
    "    \n",
    "    # Handle outliers in Amount\n",
    "    Q1 = df['Amount'].quantile(0.25)\n",
    "    Q3 = df['Amount'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df['Amount'] < lower_bound) | (df['Amount'] > upper_bound)]['Amount']\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\nFound {len(outliers)} outliers in Amount\")\n",
    "        print(\"Outlier statistics:\")\n",
    "        print(outliers.describe())\n",
    "        \n",
    "        # Cap outliers\n",
    "        df['Amount'] = df['Amount'].clip(lower_bound, upper_bound)\n",
    "        print(\"Capped outliers in Amount\")\n",
    "    \n",
    "    # Standardize all numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "    print(\"\\nStandardized numerical features\")\n",
    "    \n",
    "    # Add interaction features for V columns\n",
    "    print(\"\\nCreating interaction features...\")\n",
    "    for i in range(len(v_columns)):\n",
    "        for j in range(i+1, min(i+5, len(v_columns))):  # Create interactions with next 4 features\n",
    "            interaction_name = f\"interaction_{v_columns[i]}_{v_columns[j]}\"\n",
    "            df[interaction_name] = df[v_columns[i]] * df[v_columns[j]]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Analyzing the characteristics and distributions of our datasets:\n",
    "1. Class Distribution Analysis\n",
    "2. Feature Distributions\n",
    "3. Correlation Analysis\n",
    "4. Transaction Amount Analysis\n",
    "5. Time-based Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(df, target_col, title):\n",
    "    \"\"\"Plot the distribution of fraud vs non-fraud cases\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    class_counts = df[target_col].value_counts()\n",
    "    sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "    plt.title(f'Class Distribution - {title}')\n",
    "    plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Add percentage labels\n",
    "    total = class_counts.sum()\n",
    "    for i, count in enumerate(class_counts):\n",
    "        percentage = count/total * 100\n",
    "        plt.text(i, count, f'{percentage:.2f}%', ha='center', va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nClass distribution for {title}:\")\n",
    "    print(f\"Normal transactions: {class_counts[0]:,} ({class_counts[0]/total*100:.2f}%)\")\n",
    "    print(f\"Fraudulent transactions: {class_counts[1]:,} ({class_counts[1]/total*100:.2f}%)\")\n",
    "\n",
    "def plot_amount_distribution(df, amount_col, target_col, title):\n",
    "    \"\"\"Plot the distribution of transaction amounts\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Create subplot for normal transactions\n",
    "    plt.subplot(1, 2, 1)\n",
    "    normal_amounts = df[df[target_col] == 0][amount_col]\n",
    "    sns.histplot(normal_amounts, bins=50)\n",
    "    plt.title(f'Amount Distribution - Normal Transactions\\n{title}')\n",
    "    plt.xlabel('Transaction Amount')\n",
    "    \n",
    "    # Create subplot for fraudulent transactions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    fraud_amounts = df[df[target_col] == 1][amount_col]\n",
    "    sns.histplot(fraud_amounts, bins=50, color='red')\n",
    "    plt.title(f'Amount Distribution - Fraudulent Transactions\\n{title}')\n",
    "    plt.xlabel('Transaction Amount')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\nAmount statistics for {title}:\")\n",
    "    print(\"\\nNormal Transactions:\")\n",
    "    print(normal_amounts.describe())\n",
    "    print(\"\\nFraudulent Transactions:\")\n",
    "    print(fraud_amounts.describe())\n",
    "\n",
    "def plot_correlation_matrix(df, title, exclude_cols=None):\n",
    "    \"\"\"Plot correlation matrix for numerical features\"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    # Select numerical columns\n",
    "    num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    num_cols = [col for col in num_cols if col not in exclude_cols]\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[num_cols].corr()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title(f'Correlation Matrix - {title}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot EDA for Banking Fraud Dataset\n",
    "print(\"Analyzing Banking Fraud Dataset...\")\n",
    "plot_class_distribution(banking_df, 'fraud', 'Banking Fraud')\n",
    "plot_amount_distribution(banking_df, 'amount', 'fraud', 'Banking Fraud')\n",
    "plot_correlation_matrix(banking_df, 'Banking Fraud')\n",
    "\n",
    "# Plot EDA for Credit Card Fraud Dataset\n",
    "print(\"\\nAnalyzing Credit Card Fraud Dataset...\")\n",
    "plot_class_distribution(credit_df, 'Class', 'Credit Card Fraud')\n",
    "plot_amount_distribution(credit_df, 'Amount', 'Class', 'Credit Card Fraud')\n",
    "plot_correlation_matrix(credit_df, 'Credit Card Fraud', exclude_cols=['Time'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Additional EDA: Time-based Analysis\n",
    "\n",
    "Analyzing patterns in transaction timing and frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_patterns(df, time_col, target_col, title):\n",
    "    \"\"\"Analyze and plot time-based patterns in transactions\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"No time column found in {title} dataset\")\n",
    "        return\n",
    "        \n",
    "    # Convert time to hours for credit card dataset\n",
    "    if time_col == 'Time':\n",
    "        df['Hour'] = (df[time_col] / 3600).astype(int) % 24\n",
    "        time_col = 'Hour'\n",
    "    \n",
    "    # Create figure directly\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Transaction frequency by hour\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    hourly_counts = df.groupby(time_col).size()\n",
    "    \n",
    "    # Check index type explicitly without using hasattr\n",
    "    index_type = str(hourly_counts.index.dtype)\n",
    "    if 'datetime' in index_type:\n",
    "        sns.lineplot(x=hourly_counts.index, y=hourly_counts.values, marker='o', linewidth=2, ax=ax1)\n",
    "        \n",
    "        # Rotate labels and use concise time format\n",
    "        ax1.tick_params(axis='x', rotation=45, labelright=True)\n",
    "        # Format to show only hours:minutes\n",
    "\n",
    "        ax1.xaxis.set_major_formatter(DateFormatter('%H:%M'))\n",
    "        ax1.xaxis.set_major_locator(HourLocator(interval=2))\n",
    "    else:\n",
    "        # For numeric hour values\n",
    "        sns.lineplot(x=hourly_counts.index, y=hourly_counts.values, marker='o', \n",
    "                    color='steelblue', linewidth=2, ax=ax1)\n",
    "        ax1.set_xticks(range(0, 24, 2))  # Show every 2 hours\n",
    "    \n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.set_title(f'Transaction Frequency by Hour\\n{title}', fontweight='bold')\n",
    "    ax1.set_xlabel('Hour of Day')\n",
    "    ax1.set_ylabel('Number of Transactions')\n",
    "    \n",
    "    # Plot 2: Fraud rate by hour\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    fraud_by_hour = df.groupby(time_col)[target_col].mean()\n",
    "    sns.lineplot(x=fraud_by_hour.index, y=fraud_by_hour.values, color='red', ax=ax2)\n",
    "    ax2.set_title(f'Fraud Rate by Hour\\n{title}')\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_ylabel('Fraud Rate')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Analyze time patterns for Credit Card Fraud Dataset\n",
    "print(\"Analyzing time patterns in Credit Card Fraud Dataset...\")\n",
    "credit_time_fig = analyze_time_patterns(credit_df, 'Time', 'Class', 'Credit Card Fraud')\n",
    "\n",
    "# For Banking Fraud dataset, we'll check if there's a timestamp column\n",
    "print(\"\\nChecking time patterns in Banking Fraud Dataset...\")\n",
    "banking_time_fig = analyze_time_patterns(banking_df, 'Transaction_Time', 'Is_Fraud', 'Banking Fraud')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating new features to improve model performance:\n",
    "1. Time-based features\n",
    "2. Amount-based features\n",
    "3. Statistical aggregations\n",
    "4. Interaction features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_time_features(df, time_col='Time'):\n",
    "    \"\"\"\n",
    "    Create time-based features from timestamp\n",
    "    \"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # For credit card data (Time is in seconds)\n",
    "    if time_col == 'Time':\n",
    "        df = df.copy()\n",
    "        # Hour of day\n",
    "        df['hour'] = (df[time_col] / 3600).astype(int) % 24\n",
    "        # Part of day (morning, afternoon, evening, night)\n",
    "        df['part_of_day'] = df['hour'].map(\n",
    "            lambda x: 'night' if x < 6 else \n",
    "            'morning' if x < 12 else \n",
    "            'afternoon' if x < 18 else 'evening'\n",
    "        )\n",
    "        # Is weekend (assuming Time starts from beginning of week)\n",
    "        df['is_weekend'] = ((df[time_col] / (3600 * 24)).astype(int) % 7).map(lambda x: 1 if x >= 5 else 0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def engineer_amount_features(df, amount_col):\n",
    "    \"\"\"\n",
    "    Create amount-based features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Amount buckets\n",
    "    df['amount_bucket'] = pd.qcut(df[amount_col], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # Round amounts\n",
    "    df['amount_round'] = df[amount_col].round(-1)  # Round to nearest 10\n",
    "    \n",
    "    # Amount relative to mean\n",
    "    mean_amount = df[amount_col].mean()\n",
    "    df['amount_vs_mean'] = df[amount_col] / mean_amount\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_statistical_features(df, group_cols, agg_col='amount'):\n",
    "    \"\"\"\n",
    "    Create statistical aggregations for specified grouping\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Group by specified columns and calculate statistics\n",
    "    for col in group_cols:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate aggregations\n",
    "        aggs = df.groupby(col)[agg_col].agg(['mean', 'std', 'count']).compute()\n",
    "        \n",
    "        # Add as new features\n",
    "        df[f'{col}_mean_{agg_col}'] = df[col].map(aggs['mean'])\n",
    "        df[f'{col}_std_{agg_col}'] = df[col].map(aggs['std'])\n",
    "        df[f'{col}_count'] = df[col].map(aggs['count'])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to banking fraud dataset\n",
    "print(\"Engineering features for banking fraud dataset...\")\n",
    "banking_df = engineer_time_features(banking_df, 'timestamp')\n",
    "banking_df = engineer_amount_features(banking_df, 'amount')\n",
    "banking_df = create_statistical_features(\n",
    "    banking_df, \n",
    "    group_cols=['merchant', 'category'], \n",
    "    agg_col='amount'\n",
    ")\n",
    "\n",
    "# Apply feature engineering to credit card fraud dataset\n",
    "print(\"\\nEngineering features for credit card fraud dataset...\")\n",
    "credit_df = engineer_time_features(credit_df, 'Time')\n",
    "credit_df = engineer_amount_features(credit_df, 'Amount')\n",
    "\n",
    "# Display new features\n",
    "print(\"\\nNew features in banking fraud dataset:\")\n",
    "print(banking_df.columns.compute())\n",
    "print(\"\\nNew features in credit card fraud dataset:\")\n",
    "print(credit_df.columns.compute())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Data Balancing with SMOTE\n",
    "\n",
    "Handling class imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_balancing(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to balance the dataset\n",
    "    \"\"\"\n",
    "    # Convert Dask DataFrames to numpy arrays for SMOTE\n",
    "    if isinstance(X, dd.DataFrame):\n",
    "        X = X.compute()\n",
    "    if isinstance(y, dd.Series):\n",
    "        y = y.compute()\n",
    "        \n",
    "    # Initialize and apply SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Print balancing results\n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    # Convert back to Dask arrays for distributed processing\n",
    "    X_balanced_dask = da.from_array(X_balanced, chunks='auto')\n",
    "    y_balanced_dask = da.from_array(y_balanced, chunks='auto')\n",
    "    \n",
    "    return X_balanced_dask, y_balanced_dask\n",
    "\n",
    "# Prepare features for SMOTE\n",
    "def prepare_features_for_smote(df, target_col, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Prepare features for SMOTE by handling categorical variables\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    # Remove target and excluded columns\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col] + exclude_cols]\n",
    "    \n",
    "    # Convert categorical columns to numeric\n",
    "    X = df[feature_cols].copy()\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "    \n",
    "    return X, df[target_col]\n",
    "\n",
    "# Apply SMOTE to banking fraud dataset\n",
    "print(\"Applying SMOTE to banking fraud dataset...\")\n",
    "X_bank, y_bank = prepare_features_for_smote(\n",
    "    banking_df, \n",
    "    target_col='fraud',\n",
    "    exclude_cols=['timestamp']  # Exclude any non-feature columns\n",
    ")\n",
    "X_bank_balanced, y_bank_balanced = apply_smote_balancing(X_bank, y_bank)\n",
    "\n",
    "# Apply SMOTE to credit card fraud dataset\n",
    "print(\"\\nApplying SMOTE to credit card fraud dataset...\")\n",
    "X_credit, y_credit = prepare_features_for_smote(\n",
    "    credit_df, \n",
    "    target_col='Class',\n",
    "    exclude_cols=['Time']  # Exclude any non-feature columns\n",
    ")\n",
    "X_credit_balanced, y_credit_balanced = apply_smote_balancing(X_credit, y_credit)\n",
    "\n",
    "# Verify final shapes\n",
    "print(\"\\nFinal balanced dataset shapes:\")\n",
    "print(f\"Banking Fraud - X: {X_bank_balanced.shape}, y: {y_bank_balanced.shape}\")\n",
    "print(f\"Credit Card Fraud - X: {X_credit_balanced.shape}, y: {y_credit_balanced.shape}\")\n",
    "\n",
    "# Plot class distribution after SMOTE\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Banking Fraud\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y_bank_balanced)\n",
    "plt.title('Class Distribution After SMOTE\\nBanking Fraud')\n",
    "plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Credit Card Fraud\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_credit_balanced)\n",
    "plt.title('Class Distribution After SMOTE\\nCredit Card Fraud')\n",
    "plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Data Splitting and SMOTE Balancing\n",
    "\n",
    "Split the data into training and testing sets, then apply SMOTE for handling class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(df, target_col='fraud', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for training with train-test split and SMOTE balancing\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Convert to numpy arrays for SMOTE\n",
    "    X = X.compute()\n",
    "    y = y.compute()\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Convert back to Dask arrays for distributed processing\n",
    "    X_train_dask = da.from_array(X_train_balanced, chunks='auto')\n",
    "    X_test_dask = da.from_array(X_test, chunks='auto')\n",
    "    y_train_dask = da.from_array(y_train_balanced, chunks='auto')\n",
    "    y_test_dask = da.from_array(y_test, chunks='auto')\n",
    "    \n",
    "    print(f\"Training set shape: {X_train_balanced.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    print(f\"Class distribution in balanced training set:\\n{np.bincount(y_train_balanced)}\")\n",
    "    \n",
    "    return X_train_dask, X_test_dask, y_train_dask, y_test_dask\n",
    "\n",
    "# Prepare banking fraud data\n",
    "print(\"Preparing banking fraud data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_data_for_training(\n",
    "    banking_df, \n",
    "    target_col='fraud'\n",
    ")\n",
    "\n",
    "# Prepare credit card fraud data\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_data_for_training(\n",
    "    credit_df, \n",
    "    target_col='Class'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 3. Model Training with Distributed Computing\n",
    "\n",
    "Training models using Dask-ML for distributed processing. We'll implement:\n",
    "1. Distributed LightGBM\n",
    "2. Distributed Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distributed_lightgbm(X_train, y_train, X_test, y_test, dataset_name):\n",
    "    \"\"\"\n",
    "    Train LightGBM model with distributed computing support\n",
    "    \"\"\"\n",
    "    # LightGBM parameters optimized for fraud detection\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 32,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Initialize and train model with Dask wrapper\n",
    "    model = ParallelPostFit(LGBMClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    print(f\"\\nLightGBM Results for {dataset_name}:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba[:, 1]))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_distributed_random_forest(X_train, y_train, X_test, y_test, dataset_name):\n",
    "    \"\"\"\n",
    "    Train Random Forest with distributed computing support\n",
    "    \"\"\"\n",
    "    # Random Forest parameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Initialize and train model with Dask wrapper\n",
    "    model = ParallelPostFit(RandomForestClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    print(f\"\\nRandom Forest Results for {dataset_name}:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba[:, 1]))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train models on banking fraud data\n",
    "print(\"Training models on banking fraud data...\")\n",
    "lgb_bank = train_distributed_lightgbm(\n",
    "    X_train_bank, y_train_bank, \n",
    "    X_test_bank, y_test_bank,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "rf_bank = train_distributed_random_forest(\n",
    "    X_train_bank, y_train_bank, \n",
    "    X_test_bank, y_test_bank,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "# Train models on credit card fraud data\n",
    "print(\"\\nTraining models on credit card fraud data...\")\n",
    "lgb_credit = train_distributed_lightgbm(\n",
    "    X_train_credit, y_train_credit, \n",
    "    X_test_credit, y_test_credit,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n",
    "\n",
    "rf_credit = train_distributed_random_forest(\n",
    "    X_train_credit, y_train_credit, \n",
    "    X_test_credit, y_test_credit,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Analysis\n",
    "\n",
    "Analyzing model performance and generating visualizations for both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with seaborn\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, title):\n",
    "    \"\"\"\n",
    "    Plot ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {title}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for banking fraud models\n",
    "print(\"Plotting results for banking fraud models...\")\n",
    "\n",
    "# LightGBM\n",
    "y_pred_lgb_bank = lgb_bank.predict(X_test_bank)\n",
    "y_pred_proba_lgb_bank = lgb_bank.predict_proba(X_test_bank)[:, 1]\n",
    "plot_confusion_matrix(y_test_bank, y_pred_lgb_bank, \"Banking Fraud - LightGBM\")\n",
    "plot_roc_curve(y_test_bank, y_pred_proba_lgb_bank, \"Banking Fraud - LightGBM\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf_bank = rf_bank.predict(X_test_bank)\n",
    "y_pred_proba_rf_bank = rf_bank.predict_proba(X_test_bank)[:, 1]\n",
    "plot_confusion_matrix(y_test_bank, y_pred_rf_bank, \"Banking Fraud - Random Forest\")\n",
    "plot_roc_curve(y_test_bank, y_pred_proba_rf_bank, \"Banking Fraud - Random Forest\")\n",
    "\n",
    "# Plot results for credit card fraud models\n",
    "print(\"\\nPlotting results for credit card fraud models...\")\n",
    "\n",
    "# LightGBM\n",
    "y_pred_lgb_credit = lgb_credit.predict(X_test_credit)\n",
    "y_pred_proba_lgb_credit = lgb_credit.predict_proba(X_test_credit)[:, 1]\n",
    "plot_confusion_matrix(y_test_credit, y_pred_lgb_credit, \"Credit Card Fraud - LightGBM\")\n",
    "plot_roc_curve(y_test_credit, y_pred_proba_lgb_credit, \"Credit Card Fraud - LightGBM\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf_credit = rf_credit.predict(X_test_credit)\n",
    "y_pred_proba_rf_credit = rf_credit.predict_proba(X_test_credit)[:, 1]\n",
    "plot_confusion_matrix(y_test_credit, y_pred_rf_credit, \"Credit Card Fraud - Random Forest\")\n",
    "plot_roc_curve(y_test_credit, y_pred_proba_rf_credit, \"Credit Card Fraud - Random Forest\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Analyzing which features contribute most to fraud detection in both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"\n",
    "    Plot feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    if isinstance(model, ParallelPostFit):\n",
    "        importance = model.estimator.feature_importances_\n",
    "    else:\n",
    "        importance = model.feature_importances_\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Important Features - {title}')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "banking_features = banking_df.drop('fraud', axis=1).columns\n",
    "credit_features = credit_df.drop('Class', axis=1).columns\n",
    "\n",
    "# Plot feature importance for banking fraud models\n",
    "print(\"Feature importance analysis for banking fraud models...\")\n",
    "plot_feature_importance(lgb_bank, banking_features, \"Banking Fraud - LightGBM\")\n",
    "plot_feature_importance(rf_bank, banking_features, \"Banking Fraud - Random Forest\")\n",
    "\n",
    "# Plot feature importance for credit card fraud models\n",
    "print(\"\\nFeature importance analysis for credit card fraud models...\")\n",
    "plot_feature_importance(lgb_credit, credit_features, \"Credit Card Fraud - LightGBM\")\n",
    "plot_feature_importance(rf_credit, credit_features, \"Credit Card Fraud - Random Forest\")\n",
    "\n",
    "# Clean up Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 5. Model Architecture & Training\n",
    "\n",
    "Model training pipeline:\n",
    "1. Dimensionality Reduction (PCA)\n",
    "2. Gradient Boosting with Feature Selection\n",
    "3. Model Ensemble:\n",
    "   - Random Forest\n",
    "   - LightGBM\n",
    "   - Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalFraudDetector:\n",
    "    \"\"\"Ensemble system combining multiple fraud detection approaches.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Rapid pattern detector for numerical features\n",
    "        self.pattern_detector = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5\n",
    "        )\n",
    "        \n",
    "        # Feature interaction analyzer\n",
    "        self.interaction_analyzer = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Anomaly detector for unusual patterns\n",
    "        self.anomaly_detector = self.build_autoencoder()\n",
    "        \n",
    "        # Text analyzer for transaction descriptions\n",
    "        self.text_analyzer = self.build_finbert()\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        \"\"\"Constructs autoencoder for anomaly detection.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            # Encoder layers\n",
    "            keras.layers.Dense(64, activation='relu', name='encoder_1'),\n",
    "            keras.layers.Dense(32, activation='relu', name='encoder_2'),\n",
    "            \n",
    "            # Bottleneck layer\n",
    "            keras.layers.Dense(16, activation='relu', name='bottleneck'),\n",
    "            \n",
    "            # Decoder layers\n",
    "            keras.layers.Dense(32, activation='relu', name='decoder_1'),\n",
    "            keras.layers.Dense(64, activation='relu', name='decoder_2'),\n",
    "            keras.layers.Dense(1, activation='sigmoid', name='output')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_finbert(self):\n",
    "        \"\"\"Initializes FinBERT for financial text analysis.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_system(self, X_train, y_train):\n",
    "        \"\"\"Trains all components of the fraud detection system.\"\"\"\n",
    "        print(\"Training pattern detector...\")\n",
    "        self.pattern_detector.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Training interaction analyzer...\")\n",
    "        self.interaction_analyzer.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Training anomaly detector...\")\n",
    "        self.anomaly_detector.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Text analyzer training would go here if text data available\n",
    "    \n",
    "    def analyze_transaction(self, transaction):\n",
    "        \"\"\"Combines all analysis components for final fraud prediction.\"\"\"\n",
    "        # Get component predictions\n",
    "        pattern_score = self.pattern_detector.predict_proba(transaction)[0][1]\n",
    "        interaction_score = self.interaction_analyzer.predict_proba(transaction)[0][1]\n",
    "        anomaly_score = self.anomaly_detector.predict(transaction)[0][0]\n",
    "        \n",
    "        # Weighted combination based on component reliability\n",
    "        final_score = (\n",
    "            0.4 * pattern_score +      # LightGBM for quick pattern detection\n",
    "            0.35 * interaction_score + # Random Forest for feature interactions\n",
    "            0.25 * anomaly_score      # Autoencoder for anomaly detection\n",
    "        )\n",
    "        \n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(data: pd.DataFrame, title: str) -> None:\n",
    "    \"\"\"Analyzes transaction timing patterns using modern matplotlib.\"\"\"\n",
    "    \n",
    "    # Create figure and axis objects\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Calculate distributions\n",
    "    fraud_hourly = data[data['is_fraud'] == 1]['transaction_hour'].value_counts().sort_index()\n",
    "    normal_hourly = data[data['is_fraud'] == 0]['transaction_hour'].value_counts().sort_index()\n",
    "    \n",
    "    # Normalize distributions\n",
    "    fraud_hourly = fraud_hourly / fraud_hourly.sum()\n",
    "    normal_hourly = normal_hourly / normal_hourly.sum()\n",
    "    \n",
    "    # Plot patterns using modern syntax\n",
    "    ax.plot(fraud_hourly.index, fraud_hourly.values, \n",
    "           label='Fraudulent', color=COLORS['fraud'])\n",
    "    ax.plot(normal_hourly.index, normal_hourly.values, \n",
    "           label='Normal', color=COLORS['normal'])\n",
    "    \n",
    "    # Configure axis\n",
    "    ax.set_title(f\"{title}\\nHourly Transaction Patterns\")\n",
    "    ax.set_xlabel(\"Hour of Day\")\n",
    "    ax.set_ylabel(\"Transaction Frequency\")\n",
    "    ax.legend(frameon=True, facecolor='white', framealpha=0.9)\n",
    "    \n",
    "    # Save and close\n",
    "    plt.savefig(f'figures/{title.lower().replace(\" \", \"_\")}_time_patterns.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def analyze_amount_distribution(data: pd.DataFrame, title: str) -> None:\n",
    "    \"\"\"Analyzes transaction amount patterns using modern matplotlib.\"\"\"\n",
    "    \n",
    "    # Create figure and axis objects\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax.hist(data[data['is_fraud'] == 0]['amount'], \n",
    "            bins=np.linspace(0, data['amount'].quantile(0.99), 50),\n",
    "            alpha=0.6, label='Normal', color=COLORS['normal'])\n",
    "    ax.hist(data[data['is_fraud'] == 1]['amount'], \n",
    "            bins=np.linspace(0, data['amount'].quantile(0.99), 50),\n",
    "            alpha=0.6, label='Fraudulent', color=COLORS['fraud'])\n",
    "    \n",
    "    # Configure axis\n",
    "    ax.set_title(f\"{title}\\nTransaction Amount Distribution\")\n",
    "    ax.set_xlabel(\"Transaction Amount ($)\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.legend(frameon=True, facecolor='white', framealpha=0.9)\n",
    "    \n",
    "    # Use log scale for better distribution visibility\n",
    "    if data['amount'].max() / data['amount'].min() > 1000:\n",
    "        ax.set_yscale('log')\n",
    "    \n",
    "    # Save and close\n",
    "    plt.savefig(f'figures/{title.lower().replace(\" \", \"_\")}_distribution.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "# Generate visualizations\n",
    "for dataset, title in [(ieee_data, \"Banking Transactions\"), \n",
    "                      (credit_data, \"Credit Card Transactions\")]:\n",
    "    analyze_temporal_patterns(dataset, title)\n",
    "    analyze_amount_distribution(dataset, title)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation\n",
    "\n",
    "We'll now:\n",
    "1. Load and preprocess the transaction data\n",
    "2. Split into training and testing sets\n",
    "3. Train the multi-modal system\n",
    "4. Evaluate performance using standard metrics\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 4. Pattern Analysis and Visualization\n",
    "\n",
    "We'll analyze and visualize:\n",
    "1. Temporal patterns in fraudulent transactions\n",
    "2. Amount distribution analysis\n",
    "3. Feature importance across models\n",
    "4. Anomaly detection thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(data, title):\n",
    "    \"\"\"Analyzes transaction timing patterns.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Analyze hourly patterns\n",
    "    fraud_hourly = data[data['is_fraud'] == 1]['transaction_hour'].value_counts().sort_index()\n",
    "    normal_hourly = data[data['is_fraud'] == 0]['transaction_hour'].value_counts().sort_index()\n",
    "    \n",
    "    # Normalize distributions\n",
    "    fraud_hourly = fraud_hourly / fraud_hourly.sum()\n",
    "    normal_hourly = normal_hourly / normal_hourly.sum()\n",
    "    \n",
    "    # Plot patterns\n",
    "    plt.plot(fraud_hourly.index, fraud_hourly.values, \n",
    "            label='Fraudulent', color='#e74c3c', linewidth=2)\n",
    "    plt.plot(normal_hourly.index, normal_hourly.values, \n",
    "            label='Normal', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    plt.title(f\"{title}\\nHourly Transaction Patterns\", fontsize=12)\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Transaction Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def analyze_amount_distribution(data, title):\n",
    "    \"\"\"Analyzes transaction amount patterns.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.hist(data[data['is_fraud'] == 0]['amount'], \n",
    "            bins=50, alpha=0.5, label='Normal', \n",
    "            color='#2ecc71')\n",
    "    plt.hist(data[data['is_fraud'] == 1]['amount'], \n",
    "            bins=50, alpha=0.5, label='Fraudulent', \n",
    "            color='#e74c3c')\n",
    "    \n",
    "    plt.title(f\"{title}\\nTransaction Amount Distribution\", fontsize=12)\n",
    "    plt.xlabel(\"Transaction Amount ($)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Generate visualizations\n",
    "analyze_temporal_patterns(ieee_data, \"Banking Transactions\")\n",
    "plt.savefig('figures/banking_time_patterns.png')\n",
    "plt.close()\n",
    "\n",
    "analyze_temporal_patterns(credit_data, \"Credit Card Transactions\")\n",
    "plt.savefig('figures/credit_time_patterns.png')\n",
    "plt.close()\n",
    "\n",
    "analyze_amount_distribution(ieee_data, \"Banking Transactions\")\n",
    "plt.savefig('figures/banking_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "analyze_amount_distribution(credit_data, \"Credit Card Transactions\")\n",
    "plt.savefig('figures/credit_distribution.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Our Fraud Detection Dream Team\n",
    "\n",
    "Now we'll set up our ensemble of fraud detectors. Each one is good at spotting different types of suspicious activity. It's like having a team of detectives, each with their own special skills:\n",
    "\n",
    "1. The Quick Spotter (LightGBM) - Our speed demon who can quickly spot obvious fraud patterns\n",
    "2. The Pattern Master (Random Forest) - Our experienced detective who's great at seeing the big picture\n",
    "3. The Deep Thinker (Neural Network) - Our analyst who can spot the most subtle patterns\n",
    "\n",
    "Together, they make an unstoppable fraud-fighting team! \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Let's Put It All Together!\n",
    "\n",
    "Time to see our fraud detection team in action! We'll:\n",
    "1. Load our transaction data\n",
    "2. Train our team of detectives\n",
    "3. Test how well they do at catching the bad guys\n",
    "\n",
    "Remember: In the real world, catching fraud is super important. Every fraudulent transaction we stop means someone's money stays safe! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our transaction data\n",
    "print(\" Loading our transaction data...\")\n",
    "ieee_data = load_transaction_data('data/input/banking-fraud/transactions.csv')\n",
    "credit_data = load_transaction_data('data/input/creditcard-fraud/transactions.csv')\n",
    "\n",
    "# Create and train our fraud detection team\n",
    "print(\"\\n Assembling our fraud detection team...\")\n",
    "fraud_team = FraudDetectionTeam()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n Splitting our data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ieee_data.drop('is_fraud', axis=1),  # Features\n",
    "    ieee_data['is_fraud'],               # What we're trying to predict\n",
    "    test_size=0.2,                         # Use 20% for testing\n",
    "    random_state=42                        # For reproducible results\n",
    ")\n",
    "\n",
    "# Train our team\n",
    "print(\"\\n Training our fraud fighters...\")\n",
    "fraud_team.train_team(X_train, y_train)\n",
    "\n",
    "# Test how well we do\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n Testing our team's fraud-catching skills...\")\n",
    "test_predictions = [\n",
    "    fraud_team.detect_fraud(transaction.reshape(1, -1)) > 0.5 \n",
    "    for transaction in X_test.values\n",
    "]\n",
    "\n",
    "print(\"\\n Here's how well our team did:\")\n",
    "print(classification_report(y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Visualizing Our Results\n",
    "\n",
    "Let's create some pretty visualizations to understand how our fraud detection team is doing. Remember: a picture is worth a thousand words (or in our case, thousands of transactions)! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fraud_patterns(data, title):\n",
    "    \"\"\"Creates a nice visualization of transaction patterns.\n",
    "    \n",
    "    Shows when fraudsters are most active!\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot hour of day patterns\n",
    "    fraud_by_hour = data[data['is_fraud'] == 1]['hour_of_day'].value_counts().sort_index()\n",
    "    normal_by_hour = data[data['is_fraud'] == 0]['hour_of_day'].value_counts().sort_index()\n",
    "    \n",
    "    # Normalize the values\n",
    "    fraud_by_hour = fraud_by_hour / fraud_by_hour.sum()\n",
    "    normal_by_hour = normal_by_hour / normal_by_hour.sum()\n",
    "    \n",
    "    plt.plot(fraud_by_hour.index, fraud_by_hour.values, \n",
    "            label='Fraudulent', color='#e74c3c', linewidth=3)\n",
    "    plt.plot(normal_by_hour.index, normal_by_hour.values, \n",
    "            label='Normal', color='#2ecc71', linewidth=3)\n",
    "    \n",
    "    plt.title(f\"{title}\\nTransaction Patterns by Hour\", fontsize=14, pad=20)\n",
    "    plt.xlabel(\"Hour of Day\", fontsize=12)\n",
    "    plt.ylabel(\"Proportion of Transactions\", fontsize=12)\n",
    "    \n",
    "    # Add some helpful annotations\n",
    "    if title == \"Banking Transactions\":\n",
    "        plt.annotate(\"Fraudsters love\\nthe night! \", \n",
    "                    xy=(3, fraud_by_hour[3]), \n",
    "                    xytext=(3, fraud_by_hour[3] + 0.1),\n",
    "                    ha='center', va='bottom',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.3),\n",
    "                    arrowprops=dict(arrowstyle='->'))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Create our visualizations\n",
    "plot_fraud_patterns(ieee_data, \"Banking Transactions\")\n",
    "plt.savefig('figures/banking_time_patterns.png')\n",
    "plt.close()\n",
    "\n",
    "plot_fraud_patterns(credit_data, \"Credit Card Transactions\")\n",
    "plt.savefig('figures/credit_time_patterns.png')\n",
    "plt.close()\n",
    "\n",
    "# Now let's look at transaction amounts\n",
    "def plot_amount_distribution(data, title):\n",
    "    \"\"\"Shows how much money fraudsters typically try to steal.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create separate plots for normal and fraudulent transactions\n",
    "    plt.hist(data[data['is_fraud'] == 0]['amount'], \n",
    "            bins=50, alpha=0.5, label='Normal', \n",
    "            color='#2ecc71')\n",
    "    plt.hist(data[data['is_fraud'] == 1]['amount'], \n",
    "            bins=50, alpha=0.5, label='Fraudulent', \n",
    "            color='#e74c3c')\n",
    "    \n",
    "    plt.title(f\"{title}\\nTransaction Amount Distribution\", fontsize=14, pad=20)\n",
    "    plt.xlabel(\"Transaction Amount ($)\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Transactions\", fontsize=12)\n",
    "    \n",
    "    # Add some interesting annotations\n",
    "    if title == \"Banking Transactions\":\n",
    "        plt.annotate(\"Small test\\ntransactions \", \n",
    "                    xy=(10, plt.gca().get_ylim()[1]/2),\n",
    "                    xytext=(50, plt.gca().get_ylim()[1]/2),\n",
    "                    ha='left', va='center',\n",
    "                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.3),\n",
    "                    arrowprops=dict(arrowstyle='->'))\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Create amount distribution plots\n",
    "plot_amount_distribution(ieee_data, \"Banking Transactions\")\n",
    "plt.savefig('figures/banking_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "plot_amount_distribution(credit_data, \"Credit Card Transactions\")\n",
    "plt.savefig('figures/credit_distribution.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benford's Law Analysis\n",
    "\n",
    "def get_first_digit(x: float) -> int:\n",
    "    \"\"\"Get first digit of a number\"\"\"\n",
    "    # Convert to string and get first digit\n",
    "    str_x = str(abs(float(x)))\n",
    "    # Find first non-zero digit\n",
    "    for char in str_x:\n",
    "        if char.isdigit() and char != '0':\n",
    "            return int(char)\n",
    "    return None\n",
    "\n",
    "def analyze_benford(\n",
    "    df: dd.DataFrame,\n",
    "    amount_col: str,\n",
    "    fraud_col: str = 'fraud'\n",
    ") -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Analyze amount distributions using Benford's Law\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with transaction data\n",
    "        amount_col: Name of amount column\n",
    "        fraud_col: Name of fraud label column\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (legitimate_dist, fraud_dist)\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing Benford's Law patterns for {amount_col}...\")\n",
    "    \n",
    "    # Convert to pandas for this analysis (distribution calculation)\n",
    "    df = df.compute()\n",
    "    \n",
    "    # Get first digits\n",
    "    df['first_digit'] = df[amount_col].apply(get_first_digit)\n",
    "    \n",
    "    # Theoretical Benford distribution\n",
    "    benford_dist = pd.Series(\n",
    "        [np.log10(1 + 1/d) for d in range(1, 10)],\n",
    "        index=range(1, 10)\n",
    "    )\n",
    "    \n",
    "    # Calculate distributions\n",
    "    legitimate_dist = df[df[fraud_col] == 0]['first_digit'].value_counts(normalize=True).sort_index()\n",
    "    fraud_dist = df[df[fraud_col] == 1]['first_digit'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    # Plot distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(benford_dist.index, benford_dist.values, 'k--', label=\"Benford's Law\")\n",
    "    plt.plot(legitimate_dist.index, legitimate_dist.values, 'g-', label='Legitimate Transactions')\n",
    "    plt.plot(fraud_dist.index, fraud_dist.values, 'r-', label='Fraudulent Transactions')\n",
    "    \n",
    "    plt.title(f\"Benford's Law Analysis - {amount_col}\")\n",
    "    plt.xlabel('First Digit')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate deviation from Benford's Law\n",
    "    legitimate_dev = np.sqrt(np.mean((legitimate_dist - benford_dist) ** 2))\n",
    "    fraud_dev = np.sqrt(np.mean((fraud_dist - benford_dist) ** 2))\n",
    "    \n",
    "    print(f\"\\nRoot Mean Square Deviation from Benford's Law:\")\n",
    "    print(f\"Legitimate Transactions: {legitimate_dev:.4f}\")\n",
    "    print(f\"Fraudulent Transactions: {fraud_dev:.4f}\")\n",
    "    \n",
    "    return legitimate_dist, fraud_dist\n",
    "\n",
    "# Analyze each dataset\n",
    "if banking_df is not None:\n",
    "    print(\"\\nAnalyzing Banking Fraud Dataset...\")\n",
    "    banking_benford = analyze_benford(banking_processed, 'amount')\n",
    "    \n",
    "if credit_df is not None:\n",
    "    print(\"\\nAnalyzing Credit Card Fraud Dataset...\")\n",
    "    credit_benford = analyze_benford(credit_processed, 'Amount')\n",
    "    \n",
    "if ieee_df is not None and ieee_processed is not None:\n",
    "    print(\"\\nAnalyzing IEEE-CIS Fraud Dataset...\")\n",
    "    # Find transaction amount columns\n",
    "    amount_cols = [col for col in ieee_processed.columns if 'TransactionAmt' in col]\n",
    "    for col in amount_cols:\n",
    "        ieee_benford = analyze_benford(ieee_processed, col)\n",
    "\n",
    "# Add Benford's Law deviation as a feature\n",
    "if banking_df is not None:\n",
    "    banking_processed['benford_dev'] = (\n",
    "        banking_processed['amount']\n",
    "        .map_partitions(lambda x: pd.Series([get_first_digit(v) for v in x]))\n",
    "        .map_partitions(lambda x: abs(x.value_counts(normalize=True).sort_index() - benford_dist).mean())\n",
    "    )\n",
    "    \n",
    "if credit_df is not None:\n",
    "    credit_processed['benford_dev'] = (\n",
    "        credit_processed['Amount']\n",
    "        .map_partitions(lambda x: pd.Series([get_first_digit(v) for v in x]))\n",
    "        .map_partitions(lambda x: abs(x.value_counts(normalize=True).sort_index() - benford_dist).mean())\n",
    "    )\n",
    "    \n",
    "if ieee_df is not None and ieee_processed is not None:\n",
    "    for col in amount_cols:\n",
    "        ieee_processed[f'{col}_benford_dev'] = (\n",
    "            ieee_processed[col]\n",
    "            .map_partitions(lambda x: pd.Series([get_first_digit(v) for v in x]))\n",
    "            .map_partitions(lambda x: abs(x.value_counts(normalize=True).sort_index() - benford_dist).mean())\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with Advanced Techniques\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Splitting with SMOTE (After Split)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def prepare_train_test_data(\n",
    "    df: dd.DataFrame,\n",
    "    target_col: str = 'fraud',\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare train/test splits with proper SMOTE application\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of test set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Convert to numpy for sklearn\n",
    "    df = df.compute()\n",
    "    X = df.drop([target_col], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split first\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# LightGBM with Optuna Optimization\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def optimize_lightgbm(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_valid: np.ndarray,\n",
    "    y_valid: np.ndarray,\n",
    "    n_trials: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_valid: Validation features\n",
    "        y_valid: Validation labels\n",
    "        n_trials: Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "        Best parameters\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "            'min_split_gain': trial.suggest_loguniform('min_split_gain', 1e-8, 1.0),\n",
    "            'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-8, 10.0)\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"\\nBest AUC: {study.best_value:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Autoencoder for Anomaly Detection\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class FraudAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, encoding_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder with batch normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.BatchNorm1d(encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder with batch normalization\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(\n",
    "    X_train: np.ndarray,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> Tuple[FraudAutoencoder, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train autoencoder and get reconstruction error scores\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and reconstruction errors\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FraudAutoencoder(input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create data loader\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(x)\n",
    "            loss = criterion(reconstructed, x)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(loader):.6f}\")\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_tensor)\n",
    "        errors = torch.mean(torch.pow(X_tensor - reconstructed, 2), dim=1)\n",
    "        reconstruction_errors = errors.cpu().numpy()\n",
    "    \n",
    "    return model, reconstruction_errors\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing banking fraud data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_train_test_data(\n",
    "    banking_processed\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_train_test_data(\n",
    "    credit_processed\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing IEEE-CIS fraud data...\")\n",
    "X_train_ieee, X_test_ieee, y_train_ieee, y_test_ieee = prepare_train_test_data(\n",
    "    ieee_processed\n",
    ")\n",
    "\n",
    "# Train autoencoders\n",
    "print(\"\\nTraining autoencoder for banking fraud...\")\n",
    "autoencoder_bank, errors_bank = train_autoencoder(X_train_bank)\n",
    "\n",
    "print(\"\\nTraining autoencoder for credit card fraud...\")\n",
    "autoencoder_credit, errors_credit = train_autoencoder(X_train_credit)\n",
    "\n",
    "print(\"\\nTraining autoencoder for IEEE fraud...\")\n",
    "autoencoder_ieee, errors_ieee = train_autoencoder(X_train_ieee)\n",
    "\n",
    "# Add reconstruction error as a feature\n",
    "X_train_bank = np.column_stack([X_train_bank, errors_bank])\n",
    "X_test_bank = np.column_stack([\n",
    "    X_test_bank,\n",
    "    train_autoencoder(X_test_bank, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_credit = np.column_stack([X_train_credit, errors_credit])\n",
    "X_test_credit = np.column_stack([\n",
    "    X_test_credit,\n",
    "    train_autoencoder(X_test_credit, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_ieee = np.column_stack([X_train_ieee, errors_ieee])\n",
    "X_test_ieee = np.column_stack([\n",
    "    X_test_ieee,\n",
    "    train_autoencoder(X_test_ieee, epochs=1)[1]\n",
    "])\n",
    "\n",
    "# Optimize and train LightGBM models\n",
    "print(\"\\nOptimizing LightGBM for banking fraud...\")\n",
    "lgb_params_bank = optimize_lightgbm(\n",
    "    X_train_bank, y_train_bank,\n",
    "    X_test_bank, y_test_bank\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for credit card fraud...\")\n",
    "lgb_params_credit = optimize_lightgbm(\n",
    "    X_train_credit, y_train_credit,\n",
    "    X_test_credit, y_test_credit\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for IEEE fraud...\")\n",
    "lgb_params_ieee = optimize_lightgbm(\n",
    "    X_train_ieee, y_train_ieee,\n",
    "    X_test_ieee, y_test_ieee\n",
    ")\n",
    "\n",
    "# Train final models with best parameters\n",
    "lgb_bank = lgb.LGBMClassifier(**lgb_params_bank)\n",
    "lgb_bank.fit(X_train_bank, y_train_bank)\n",
    "\n",
    "lgb_credit = lgb.LGBMClassifier(**lgb_params_credit)\n",
    "lgb_credit.fit(X_train_credit, y_train_credit)\n",
    "\n",
    "lgb_ieee = lgb.LGBMClassifier(**lgb_params_ieee)\n",
    "lgb_ieee.fit(X_train_ieee, y_train_ieee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Visualization Functions\n",
    "\n",
    "@plot_with_style\n",
    "def plot_roc_curve(y_true, y_pred_proba, title_prefix=\"\"):\n",
    "    \"\"\"Plot ROC curve with enhanced styling\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix}Receiver Operating Characteristic (ROC) Curve', pad=20)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, facecolor='white', framealpha=1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_confusion_matrix(y_true, y_pred, title_prefix=\"\"):\n",
    "    \"\"\"Plot confusion matrix with enhanced styling\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with both count and percentage\n",
    "    annot = np.empty_like(cm, dtype=str)\n",
    "    np.fill_diagonal(annot, [f'{val}\\n({p:.1f}%)' for val, p in zip(np.diag(cm), np.diag(cm_percent))])\n",
    "    mask = ~np.eye(cm.shape[0], dtype=bool)\n",
    "    annot[mask] = [f'{val}\\n({p:.1f}%)' for val, p in zip(cm[mask], cm_percent[mask])]\n",
    "    \n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', cbar=True,\n",
    "                xticklabels=['Not Fraud', 'Fraud'],\n",
    "                yticklabels=['Not Fraud', 'Fraud'])\n",
    "    \n",
    "    plt.title(f'{title_prefix}Confusion Matrix', pad=20)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_feature_importance(model, feature_names, top_n=20, title_prefix=\"\"):\n",
    "    \"\"\"Plot feature importance with enhanced styling\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[-top_n:]\n",
    "    \n",
    "    plt.title(f'{title_prefix}Top {top_n} Feature Importances', pad=20)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    bars = plt.barh(range(len(indices)), importance[indices])\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_fraud_distribution(df, amount_col='amount', fraud_col='fraud', title_prefix=\"\"):\n",
    "    \"\"\"Plot fraud amount distribution with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create two subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=fraud_col, y=amount_col, data=df)\n",
    "    plt.title(f'{title_prefix}Transaction Amount Distribution by Class', pad=20)\n",
    "    plt.xlabel('Fraud (1) vs Normal (0)')\n",
    "    plt.ylabel('Amount')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data=df, x=amount_col, hue=fraud_col, multiple=\"stack\", bins=50)\n",
    "    plt.title(f'{title_prefix}Transaction Amount Histogram by Class', pad=20)\n",
    "    plt.xlabel('Amount')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_time_patterns(df, hour_col='hour', fraud_col='fraud', title_prefix=\"\"):\n",
    "    \"\"\"Plot time-based patterns with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create two subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fraud_by_hour = df.groupby([hour_col, fraud_col]).size().unstack()\n",
    "    fraud_by_hour.plot(kind='line', marker='o')\n",
    "    plt.title(f'{title_prefix}Transaction Patterns by Hour', pad=20)\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    plt.legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if 'day_of_week' in df.columns:\n",
    "        fraud_by_day = df.groupby(['day_of_week', fraud_col]).size().unstack()\n",
    "        fraud_by_day.plot(kind='bar')\n",
    "        plt.title(f'{title_prefix}Transaction Patterns by Day', pad=20)\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all visualizations for the white paper\n",
    "\n",
    "# 1. Data Distribution Analysis\n",
    "print(\"Generating data distribution plots...\")\n",
    "plot_fraud_distribution(banking_processed, title_prefix=\"Banking: \")\n",
    "plt.savefig('../docs/figures/banking_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plot_fraud_distribution(credit_processed, title_prefix=\"Credit Card: \")\n",
    "plt.savefig('../docs/figures/credit_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Time Pattern Analysis\n",
    "print(\"\\nGenerating time pattern plots...\")\n",
    "plot_time_patterns(banking_processed, title_prefix=\"Banking: \")\n",
    "plt.savefig('../docs/figures/banking_time_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plot_time_patterns(credit_processed, title_prefix=\"Credit Card: \")\n",
    "plt.savefig('../docs/figures/credit_time_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Model Performance Visualization\n",
    "print(\"\\nGenerating model performance plots...\")\n",
    "plot_roc_curve(y_test, y_pred_proba, title_prefix=\"Ensemble Model: \")\n",
    "plt.savefig('../docs/figures/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, title_prefix=\"Ensemble Model: \")\n",
    "plt.savefig('../docs/figures/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Feature Importance Analysis\n",
    "print(\"\\nGenerating feature importance plot...\")\n",
    "plot_feature_importance(model.lightgbm, X_train.columns, title_prefix=\"LightGBM: \")\n",
    "plt.savefig('../docs/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll visualizations have been generated and saved to the docs/figures directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Analysis\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Performance Metrics\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model(\n",
    "    model: Any,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    print(f\"AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"Average Precision Score: {avg_precision:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.4f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return auc_score, avg_precision\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# SHAP Analysis\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def analyze_shap_values(\n",
    "    model: Any,\n",
    "    X_test: np.ndarray,\n",
    "    feature_names: List[str],\n",
    "    dataset_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using SHAP values\n",
    "    \"\"\"\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    \n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # For binary classification\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        plot_type=\"bar\",\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Feature Importance - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed plot for top features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=feature_names,\n",
    "        max_display=10,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Summary Plot - {dataset_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate banking fraud model\n",
    "print(\"Evaluating banking fraud model...\")\n",
    "banking_auc, banking_ap = evaluate_model(\n",
    "    lgb_bank,\n",
    "    X_test_bank,\n",
    "    y_test_bank,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "# Evaluate credit card fraud model\n",
    "print(\"\\nEvaluating credit card fraud model...\")\n",
    "credit_auc, credit_ap = evaluate_model(\n",
    "    lgb_credit,\n",
    "    X_test_credit,\n",
    "    y_test_credit,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n",
    "\n",
    "# Evaluate IEEE fraud model\n",
    "print(\"\\nEvaluating IEEE fraud model...\")\n",
    "ieee_auc, ieee_ap = evaluate_model(\n",
    "    lgb_ieee,\n",
    "    X_test_ieee,\n",
    "    y_test_ieee,\n",
    "    \"IEEE-CIS Fraud\"\n",
    ")\n",
    "\n",
    "# SHAP analysis for each model\n",
    "print(\"\\nAnalyzing feature importance with SHAP...\")\n",
    "\n",
    "# Banking model\n",
    "analyze_shap_values(\n",
    "    lgb_bank,\n",
    "    X_test_bank,\n",
    "    banking_processed.drop('fraud', axis=1).columns,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "# Credit card model\n",
    "analyze_shap_values(\n",
    "    lgb_credit,\n",
    "    X_test_credit,\n",
    "    credit_processed.drop('fraud', axis=1).columns,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n",
    "\n",
    "# IEEE model\n",
    "analyze_shap_values(\n",
    "    lgb_ieee,\n",
    "    X_test_ieee,\n",
    "    ieee_processed.drop('fraud', axis=1).columns,\n",
    "    \"IEEE-CIS Fraud\"\n",
    ")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nFinal Results Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Banking Fraud AUC: {banking_auc:.4f}\")\n",
    "print(f\"Credit Card Fraud AUC: {credit_auc:.4f}\")\n",
    "print(f\"IEEE-CIS Fraud AUC: {ieee_auc:.4f}\")\n",
    "print(f\"\\nMean AUC across datasets: {np.mean([banking_auc, credit_auc, ieee_auc]):.4f}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Model Training & Evaluation\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Training Functions\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    model,\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    dataset_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train model and evaluate performance.\n",
    "    \n",
    "    Args:\n",
    "        model: Sklearn-compatible model\n",
    "        X_train, X_test: Training and test features\n",
    "        y_train, y_test: Training and test labels\n",
    "        dataset_name: Name of dataset for display\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Training Model for {dataset_name} {'='*20}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(\"\\nModel Performance:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(f\"\\nROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc_score(y_test, y_pred_proba):.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {dataset_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Train Models\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# Random Forest Models\n",
    "print(\"\\nTraining Random Forest Models...\")\n",
    "rf_banking = RandomForestClassifier(**rf_params)\n",
    "rf_credit = RandomForestClassifier(**rf_params)\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    rf_banking,\n",
    "    banking_X_train, banking_X_test,\n",
    "    banking_y_train, banking_y_test,\n",
    "    \"Banking Fraud (Random Forest)\"\n",
    ")\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    rf_credit,\n",
    "    credit_X_train, credit_X_test,\n",
    "    credit_y_train, credit_y_test,\n",
    "    \"Credit Card Fraud (Random Forest)\"\n",
    ")\n",
    "\n",
    "# LightGBM Models\n",
    "print(\"\\nTraining LightGBM Models...\")\n",
    "lgb_banking = LGBMClassifier(**lgb_params)\n",
    "lgb_credit = LGBMClassifier(**lgb_params)\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    lgb_banking,\n",
    "    banking_X_train, banking_X_test,\n",
    "    banking_y_train, banking_y_test,\n",
    "    \"Banking Fraud (LightGBM)\"\n",
    ")\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    lgb_credit,\n",
    "    credit_X_train, credit_X_test,\n",
    "    credit_y_train, credit_y_test,\n",
    "    \"Credit Card Fraud (LightGBM)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Analyzing the characteristics and distributions of our datasets:\n",
    "1. Class Distribution Analysis\n",
    "2. Feature Distributions\n",
    "3. Correlation Analysis\n",
    "4. Transaction Amount Analysis\n",
    "5. Time-based Patterns\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Additional EDA: Time-based Analysis\n",
    "\n",
    "Analyzing patterns in transaction timing and frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_patterns(df, time_col, target_col, title):\n",
    "    \"\"\"Analyze and plot time-based patterns in transactions\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"No time column found in {title} dataset\")\n",
    "        return\n",
    "        \n",
    "    # Convert time to hours for credit card dataset\n",
    "    if time_col == 'Time':\n",
    "        df['Hour'] = (df[time_col] / 3600).astype(int) % 24\n",
    "        time_col = 'Hour'\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Transaction frequency by hour\n",
    "    plt.subplot(1, 2, 1)\n",
    "    hourly_counts = df.groupby(time_col).size().compute()\n",
    "    sns.lineplot(x=hourly_counts.index, y=hourly_counts.values)\n",
    "    plt.title(f'Transaction Frequency by Hour\\n{title}')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    \n",
    "    # Plot 2: Fraud rate by hour\n",
    "    plt.subplot(1, 2, 2)\n",
    "    fraud_by_hour = df.groupby(time_col)[target_col].mean().compute()\n",
    "    sns.lineplot(x=fraud_by_hour.index, y=fraud_by_hour.values, color='red')\n",
    "    plt.title(f'Fraud Rate by Hour\\n{title}')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Fraud Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze time patterns for Credit Card Fraud Dataset\n",
    "print(\"Analyzing time patterns in Credit Card Fraud Dataset...\")\n",
    "analyze_time_patterns(credit_df, 'Time', 'Class', 'Credit Card Fraud')\n",
    "\n",
    "# For Banking Fraud dataset, we'll check if there's a timestamp column\n",
    "print(\"\\nChecking time patterns in Banking Fraud Dataset...\")\n",
    "analyze_time_patterns(banking_df, 'timestamp', 'fraud', 'Banking Fraud')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Creating new features to improve model performance:\n",
    "1. Time-based features\n",
    "2. Amount-based features\n",
    "3. Statistical aggregations\n",
    "4. Interaction features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_time_features(df, time_col='Time'):\n",
    "    \"\"\"\n",
    "    Create time-based features from timestamp\n",
    "    \"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        return df\n",
    "        \n",
    "    # For credit card data (Time is in seconds)\n",
    "    if time_col == 'Time':\n",
    "        df = df.copy()\n",
    "        # Hour of day\n",
    "        df['hour'] = (df[time_col] / 3600).astype(int) % 24\n",
    "        # Part of day (morning, afternoon, evening, night)\n",
    "        df['part_of_day'] = df['hour'].map(\n",
    "            lambda x: 'night' if x < 6 else \n",
    "            'morning' if x < 12 else \n",
    "            'afternoon' if x < 18 else 'evening'\n",
    "        )\n",
    "        # Is weekend (assuming Time starts from beginning of week)\n",
    "        df['is_weekend'] = ((df[time_col] / (3600 * 24)).astype(int) % 7).map(lambda x: 1 if x >= 5 else 0)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def engineer_amount_features(df, amount_col):\n",
    "    \"\"\"\n",
    "    Create amount-based features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Amount buckets\n",
    "    df['amount_bucket'] = pd.qcut(df[amount_col], q=5, labels=['very_low', 'low', 'medium', 'high', 'very_high'])\n",
    "    \n",
    "    # Round amounts\n",
    "    df['amount_round'] = df[amount_col].round(-1)  # Round to nearest 10\n",
    "    \n",
    "    # Amount relative to mean\n",
    "    mean_amount = df[amount_col].mean()\n",
    "    df['amount_vs_mean'] = df[amount_col] / mean_amount\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_statistical_features(df, group_cols, agg_col='amount'):\n",
    "    \"\"\"\n",
    "    Create statistical aggregations for specified grouping\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Group by specified columns and calculate statistics\n",
    "    for col in group_cols:\n",
    "        # Skip if column doesn't exist\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        # Calculate aggregations\n",
    "        aggs = df.groupby(col)[agg_col].agg(['mean', 'std', 'count']).compute()\n",
    "        \n",
    "        # Add as new features\n",
    "        df[f'{col}_mean_{agg_col}'] = df[col].map(aggs['mean'])\n",
    "        df[f'{col}_std_{agg_col}'] = df[col].map(aggs['std'])\n",
    "        df[f'{col}_count'] = df[col].map(aggs['count'])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering to banking fraud dataset\n",
    "print(\"Engineering features for banking fraud dataset...\")\n",
    "banking_df = engineer_time_features(banking_df, 'timestamp')\n",
    "banking_df = engineer_amount_features(banking_df, 'amount')\n",
    "banking_df = create_statistical_features(\n",
    "    banking_df, \n",
    "    group_cols=['merchant', 'category'], \n",
    "    agg_col='amount'\n",
    ")\n",
    "\n",
    "# Apply feature engineering to credit card fraud dataset\n",
    "print(\"\\nEngineering features for credit card fraud dataset...\")\n",
    "credit_df = engineer_time_features(credit_df, 'Time')\n",
    "credit_df = engineer_amount_features(credit_df, 'Amount')\n",
    "\n",
    "# Display new features\n",
    "print(\"\\nNew features in banking fraud dataset:\")\n",
    "print(banking_df.columns.compute())\n",
    "print(\"\\nNew features in credit card fraud dataset:\")\n",
    "print(credit_df.columns.compute())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Data Balancing with SMOTE\n",
    "\n",
    "Handling class imbalance using SMOTE (Synthetic Minority Over-sampling Technique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_balancing(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE to balance the dataset\n",
    "    \"\"\"\n",
    "    # Convert Dask DataFrames to numpy arrays for SMOTE\n",
    "    if isinstance(X, dd.DataFrame):\n",
    "        X = X.compute()\n",
    "    if isinstance(y, dd.Series):\n",
    "        y = y.compute()\n",
    "        \n",
    "    # Initialize and apply SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Print balancing results\n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    # Convert back to Dask arrays for distributed processing\n",
    "    X_balanced_dask = da.from_array(X_balanced, chunks='auto')\n",
    "    y_balanced_dask = da.from_array(y_balanced, chunks='auto')\n",
    "    \n",
    "    return X_balanced_dask, y_balanced_dask\n",
    "\n",
    "# Prepare features for SMOTE\n",
    "def prepare_features_for_smote(df, target_col, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Prepare features for SMOTE by handling categorical variables\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    # Remove target and excluded columns\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col] + exclude_cols]\n",
    "    \n",
    "    # Convert categorical columns to numeric\n",
    "    X = df[feature_cols].copy()\n",
    "    for col in X.columns:\n",
    "        if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "            X[col] = X[col].astype('category').cat.codes\n",
    "    \n",
    "    return X, df[target_col]\n",
    "\n",
    "# Apply SMOTE to banking fraud dataset\n",
    "print(\"Applying SMOTE to banking fraud dataset...\")\n",
    "X_bank, y_bank = prepare_features_for_smote(\n",
    "    banking_df, \n",
    "    target_col='fraud',\n",
    "    exclude_cols=['timestamp']  # Exclude any non-feature columns\n",
    ")\n",
    "X_bank_balanced, y_bank_balanced = apply_smote_balancing(X_bank, y_bank)\n",
    "\n",
    "# Apply SMOTE to credit card fraud dataset\n",
    "print(\"\\nApplying SMOTE to credit card fraud dataset...\")\n",
    "X_credit, y_credit = prepare_features_for_smote(\n",
    "    credit_df, \n",
    "    target_col='Class',\n",
    "    exclude_cols=['Time']  # Exclude any non-feature columns\n",
    ")\n",
    "X_credit_balanced, y_credit_balanced = apply_smote_balancing(X_credit, y_credit)\n",
    "\n",
    "# Verify final shapes\n",
    "print(\"\\nFinal balanced dataset shapes:\")\n",
    "print(f\"Banking Fraud - X: {X_bank_balanced.shape}, y: {y_bank_balanced.shape}\")\n",
    "print(f\"Credit Card Fraud - X: {X_credit_balanced.shape}, y: {y_credit_balanced.shape}\")\n",
    "\n",
    "# Plot class distribution after SMOTE\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Banking Fraud\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x=y_bank_balanced)\n",
    "plt.title('Class Distribution After SMOTE\\nBanking Fraud')\n",
    "plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Credit Card Fraud\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x=y_credit_balanced)\n",
    "plt.title('Class Distribution After SMOTE\\nCredit Card Fraud')\n",
    "plt.xlabel('Class (0: Normal, 1: Fraud)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Data Splitting and SMOTE Balancing\n",
    "\n",
    "Split the data into training and testing sets, then apply SMOTE for handling class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(df, target_col='fraud', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for training with train-test split and SMOTE balancing\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Convert to numpy arrays for SMOTE\n",
    "    X = X.compute()\n",
    "    y = y.compute()\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE to balance the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Convert back to Dask arrays for distributed processing\n",
    "    X_train_dask = da.from_array(X_train_balanced, chunks='auto')\n",
    "    X_test_dask = da.from_array(X_test, chunks='auto')\n",
    "    y_train_dask = da.from_array(y_train_balanced, chunks='auto')\n",
    "    y_test_dask = da.from_array(y_test, chunks='auto')\n",
    "    \n",
    "    print(f\"Training set shape: {X_train_balanced.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    print(f\"Class distribution in balanced training set:\\n{np.bincount(y_train_balanced)}\")\n",
    "    \n",
    "    return X_train_dask, X_test_dask, y_train_dask, y_test_dask\n",
    "\n",
    "# Prepare banking fraud data\n",
    "print(\"Preparing banking fraud data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_data_for_training(\n",
    "    banking_df, \n",
    "    target_col='fraud'\n",
    ")\n",
    "\n",
    "# Prepare credit card fraud data\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_data_for_training(\n",
    "    credit_df, \n",
    "    target_col='Class'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 3. Model Training with Distributed Computing\n",
    "\n",
    "Training models using Dask-ML for distributed processing. We'll implement:\n",
    "1. Distributed LightGBM\n",
    "2. Distributed Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distributed_lightgbm(X_train, y_train, X_test, y_test, dataset_name):\n",
    "    \"\"\"\n",
    "    Train LightGBM model with distributed computing support\n",
    "    \"\"\"\n",
    "    # LightGBM parameters optimized for fraud detection\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'n_estimators': 1000,\n",
    "        'learning_rate': 0.01,\n",
    "        'num_leaves': 32,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Initialize and train model with Dask wrapper\n",
    "    model = ParallelPostFit(LGBMClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    print(f\"\\nLightGBM Results for {dataset_name}:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba[:, 1]))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_distributed_random_forest(X_train, y_train, X_test, y_test, dataset_name):\n",
    "    \"\"\"\n",
    "    Train Random Forest with distributed computing support\n",
    "    \"\"\"\n",
    "    # Random Forest parameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Initialize and train model with Dask wrapper\n",
    "    model = ParallelPostFit(RandomForestClassifier(**params))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    print(f\"\\nRandom Forest Results for {dataset_name}:\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba[:, 1]))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train models on banking fraud data\n",
    "print(\"Training models on banking fraud data...\")\n",
    "lgb_bank = train_distributed_lightgbm(\n",
    "    X_train_bank, y_train_bank, \n",
    "    X_test_bank, y_test_bank,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "rf_bank = train_distributed_random_forest(\n",
    "    X_train_bank, y_train_bank, \n",
    "    X_test_bank, y_test_bank,\n",
    "    \"Banking Fraud\"\n",
    ")\n",
    "\n",
    "# Train models on credit card fraud data\n",
    "print(\"\\nTraining models on credit card fraud data...\")\n",
    "lgb_credit = train_distributed_lightgbm(\n",
    "    X_train_credit, y_train_credit, \n",
    "    X_test_credit, y_test_credit,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n",
    "\n",
    "rf_credit = train_distributed_random_forest(\n",
    "    X_train_credit, y_train_credit, \n",
    "    X_test_credit, y_test_credit,\n",
    "    \"Credit Card Fraud\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Analysis\n",
    "\n",
    "Analyzing model performance and generating visualizations for both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with seaborn\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {title}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_proba, title):\n",
    "    \"\"\"\n",
    "    Plot ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {title}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for banking fraud models\n",
    "print(\"Plotting results for banking fraud models...\")\n",
    "\n",
    "# LightGBM\n",
    "y_pred_lgb_bank = lgb_bank.predict(X_test_bank)\n",
    "y_pred_proba_lgb_bank = lgb_bank.predict_proba(X_test_bank)[:, 1]\n",
    "plot_confusion_matrix(y_test_bank, y_pred_lgb_bank, \"Banking Fraud - LightGBM\")\n",
    "plot_roc_curve(y_test_bank, y_pred_proba_lgb_bank, \"Banking Fraud - LightGBM\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf_bank = rf_bank.predict(X_test_bank)\n",
    "y_pred_proba_rf_bank = rf_bank.predict_proba(X_test_bank)[:, 1]\n",
    "plot_confusion_matrix(y_test_bank, y_pred_rf_bank, \"Banking Fraud - Random Forest\")\n",
    "plot_roc_curve(y_test_bank, y_pred_proba_rf_bank, \"Banking Fraud - Random Forest\")\n",
    "\n",
    "# Plot results for credit card fraud models\n",
    "print(\"\\nPlotting results for credit card fraud models...\")\n",
    "\n",
    "# LightGBM\n",
    "y_pred_lgb_credit = lgb_credit.predict(X_test_credit)\n",
    "y_pred_proba_lgb_credit = lgb_credit.predict_proba(X_test_credit)[:, 1]\n",
    "plot_confusion_matrix(y_test_credit, y_pred_lgb_credit, \"Credit Card Fraud - LightGBM\")\n",
    "plot_roc_curve(y_test_credit, y_pred_proba_lgb_credit, \"Credit Card Fraud - LightGBM\")\n",
    "\n",
    "# Random Forest\n",
    "y_pred_rf_credit = rf_credit.predict(X_test_credit)\n",
    "y_pred_proba_rf_credit = rf_credit.predict_proba(X_test_credit)[:, 1]\n",
    "plot_confusion_matrix(y_test_credit, y_pred_rf_credit, \"Credit Card Fraud - Random Forest\")\n",
    "plot_roc_curve(y_test_credit, y_pred_proba_rf_credit, \"Credit Card Fraud - Random Forest\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis\n",
    "\n",
    "Analyzing which features contribute most to fraud detection in both datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, title):\n",
    "    \"\"\"\n",
    "    Plot feature importance for tree-based models\n",
    "    \"\"\"\n",
    "    if isinstance(model, ParallelPostFit):\n",
    "        importance = model.estimator.feature_importances_\n",
    "    else:\n",
    "        importance = model.feature_importances_\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importance\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Important Features - {title}')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get feature names\n",
    "banking_features = banking_df.drop('fraud', axis=1).columns\n",
    "credit_features = credit_df.drop('Class', axis=1).columns\n",
    "\n",
    "# Plot feature importance for banking fraud models\n",
    "print(\"Feature importance analysis for banking fraud models...\")\n",
    "plot_feature_importance(lgb_bank, banking_features, \"Banking Fraud - LightGBM\")\n",
    "plot_feature_importance(rf_bank, banking_features, \"Banking Fraud - Random Forest\")\n",
    "\n",
    "# Plot feature importance for credit card fraud models\n",
    "print(\"\\nFeature importance analysis for credit card fraud models...\")\n",
    "plot_feature_importance(lgb_credit, credit_features, \"Credit Card Fraud - LightGBM\")\n",
    "plot_feature_importance(rf_credit, credit_features, \"Credit Card Fraud - Random Forest\")\n",
    "\n",
    "# Clean up Dask client\n",
    "client.close()\n",
    "cluster.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Preprocessing & Cleaning\n",
    "\n",
    "We'll standardize column names, handle missing values, and prepare the data for feature engineering.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create additional features that can help detect fraud:\n",
    "1. Temporal features (hour, day, weekend)\n",
    "2. Transaction pattern features\n",
    "3. Amount-based features\n",
    "4. Statistical aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract time-based features from timestamp.\"\"\"\n",
    "    return df.assign(\n",
    "        hour=df['timestamp'].dt.hour,\n",
    "        day_of_week=df['timestamp'].dt.dayofweek,\n",
    "        is_weekend=df['timestamp'].dt.dayofweek.isin([5, 6]).astype(int),\n",
    "        is_night=((df['timestamp'].dt.hour >= 23) | (df['timestamp'].dt.hour <= 4)).astype(int),\n",
    "        is_business_hours=((df['timestamp'].dt.hour.between(9, 17)) & \n",
    "                         ~df['timestamp'].dt.dayofweek.isin([5, 6])).astype(int)\n",
    "    )\n",
    "\n",
    "def extract_transaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract transaction pattern features.\"\"\"\n",
    "    return df.assign(\n",
    "        amount_cents=(df['amount'] * 100) % 100,\n",
    "        is_round_amount=((df['amount'] * 100) % 100 == 0).astype(int),\n",
    "        amount_log=np.log1p(df['amount'])\n",
    "    )\n",
    "\n",
    "def calculate_statistical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate statistical aggregations.\"\"\"\n",
    "    hourly_stats = df.groupby('hour')['amount'].agg(['mean', 'count']).reset_index()\n",
    "    hourly_stats.columns = ['hour', 'avg_amount_hour', 'trans_freq_hour']\n",
    "    \n",
    "    df_with_stats = df.merge(hourly_stats, on='hour', how='left')\n",
    "    return df_with_stats.assign(\n",
    "        amount_deviation=df_with_stats['amount'] - df_with_stats['avg_amount_hour'],\n",
    "        amount_hour_percentile=df_with_stats.groupby('hour')['amount'].transform(\n",
    "            lambda x: pd.qcut(x, q=10, labels=False, duplicates='drop')\n",
    "        )\n",
    "    )\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create fraud detection features using functional transformations.\"\"\"\n",
    "    return (df.copy()\n",
    "            .pipe(extract_temporal_features)\n",
    "            .pipe(extract_transaction_features)\n",
    "            .pipe(calculate_statistical_features))\n",
    "\n",
    "# Apply feature engineering\n",
    "banking_featured = engineer_features(banking_clean)\n",
    "credit_featured = engineer_features(credit_clean)\n",
    "\n",
    "# Display feature summary\n",
    "feature_summary = pd.DataFrame({\n",
    "    'Feature': banking_featured.columns,\n",
    "    'Non-Null Count': banking_featured.count(),\n",
    "    'Mean': banking_featured.mean(numeric_only=True),\n",
    "    'Std': banking_featured.std(numeric_only=True)\n",
    "}).dropna()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Data Balancing with SMOTE\n",
    "\n",
    "Since fraud is a rare event, we'll use SMOTE to balance the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns() -> List[str]:\n",
    "    \"\"\"Return list of features used for training.\"\"\"\n",
    "    return [\n",
    "        'amount', 'amount_log', 'hour', 'is_weekend', 'is_night',\n",
    "        'is_business_hours', 'is_round_amount', 'amount_hour_percentile',\n",
    "        'amount_deviation', 'trans_freq_hour'\n",
    "    ]\n",
    "\n",
    "def split_and_scale_data(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Split data and scale features.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    return (scaler.fit_transform(X_train), \n",
    "            scaler.transform(X_test),\n",
    "            y_train, y_test)\n",
    "\n",
    "def balance_with_smote(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Apply SMOTE to balance the dataset.\"\"\"\n",
    "    smote = SMOTE(random_state=42)\n",
    "    return smote.fit_resample(X, y)\n",
    "\n",
    "def prepare_training_data(df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for model training using functional approach.\"\"\"\n",
    "    feature_cols = get_feature_columns()\n",
    "    X, y = df[feature_cols].values, df['fraud'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_and_scale_data(X, y)\n",
    "    X_train_balanced, y_train_balanced = balance_with_smote(X_train, y_train)\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "# Prepare datasets\n",
    "training_data = {\n",
    "    'banking': prepare_training_data(banking_featured),\n",
    "    'credit': prepare_training_data(credit_featured)\n",
    "}\n",
    "\n",
    "# Create data summary\n",
    "data_summary = pd.DataFrame({\n",
    "    'Dataset': ['Banking', 'Credit'],\n",
    "    'Training Samples': [training_data['banking'][0].shape[0], \n",
    "                        training_data['credit'][0].shape[0]],\n",
    "    'Testing Samples': [training_data['banking'][1].shape[0], \n",
    "                       training_data['credit'][1].shape[0]],\n",
    "    'Features': [training_data['banking'][0].shape[1], \n",
    "                training_data['credit'][0].shape[1]],\n",
    "    'Balanced Ratio': [np.mean(training_data['banking'][2]), \n",
    "                      np.mean(training_data['credit'][2])]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    \"\"\"Pipeline for dimensionality reduction, feature selection, and model training.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        self.config = config\n",
    "        self.pca = PCA(n_components=config.pca_n_components, random_state=RANDOM_SEED)\n",
    "        self.feature_selector = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.models = {}\n",
    "        \n",
    "    def fit_transform_pca(self, X_train: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply PCA keeping components that explain n_components variance.\"\"\"\n",
    "        # Scale data first\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Apply PCA\n",
    "        X_train_pca = self.pca.fit_transform(X_train_scaled)\n",
    "        X_test_pca = self.pca.transform(X_test_scaled)\n",
    "        \n",
    "        return X_train_pca, X_test_pca\n",
    "    \n",
    "    def select_features(self, X: np.ndarray, y: np.ndarray, is_training: bool = True) -> np.ndarray:\n",
    "        \"\"\"Use gradient boosting for feature selection.\"\"\"\n",
    "        if is_training or self.feature_selector is None:\n",
    "            gbdt = GradientBoostingClassifier(\n",
    "                n_estimators=self.config.gbdt_n_estimators,\n",
    "                random_state=RANDOM_SEED\n",
    "            )\n",
    "            self.feature_selector = SelectFromModel(gbdt, prefit=False)\n",
    "            return self.feature_selector.fit_transform(X, y)\n",
    "        return self.feature_selector.transform(X)\n",
    "    \n",
    "    def create_neural_network(self, input_dim: int) -> Model:\n",
    "        \"\"\"Create neural network with batch normalization and early stopping.\"\"\"\n",
    "        model = Sequential([\n",
    "            Dense(self.config.nn_units[0], activation='relu', input_dim=input_dim),\n",
    "            BatchNormalization(),\n",
    "            Dropout(self.config.nn_dropout_rates[0]),\n",
    "            Dense(self.config.nn_units[1], activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(self.config.nn_dropout_rates[1]),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.nn_learning_rate),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', AUC(name='auc')]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_models(self, X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"Train and return multiple models with early stopping and proper validation.\"\"\"\n",
    "        # Random Forest\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=self.config.rf_n_estimators,\n",
    "            max_depth=self.config.rf_max_depth,\n",
    "            random_state=RANDOM_SEED,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1  # Use all CPU cores\n",
    "        ).fit(X_train, y_train)\n",
    "        \n",
    "        # LightGBM\n",
    "        train_data = lgb.Dataset(X_train, y_train)\n",
    "        valid_data = lgb.Dataset(X_test, y_train)  # For early stopping\n",
    "        \n",
    "        lgb_model = lgb.train(\n",
    "            params={\n",
    "                'objective': 'binary',\n",
    "                'metric': ['auc', 'binary_logloss'],\n",
    "                'num_leaves': self.config.lgb_num_leaves,\n",
    "                'learning_rate': self.config.lgb_learning_rate,\n",
    "                'feature_fraction': self.config.lgb_feature_fraction,\n",
    "                'random_state': RANDOM_SEED,\n",
    "                'verbose': -1\n",
    "            },\n",
    "            train_set=train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            num_boost_round=self.config.lgb_num_boost_round,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Neural Network\n",
    "        nn_model = self.create_neural_network(X_train.shape[1])\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            mode='max'\n",
    "        )\n",
    "        \n",
    "        nn_model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=self.config.nn_epochs,\n",
    "            batch_size=self.config.nn_batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        self.models = {\n",
    "            'random_forest': {\n",
    "                'model': rf_model,\n",
    "                'predictions': rf_model.predict_proba(X_test)[:, 1]\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'model': lgb_model,\n",
    "                'predictions': lgb_model.predict(X_test)\n",
    "            },\n",
    "            'neural_network': {\n",
    "                'model': nn_model,\n",
    "                'predictions': nn_model.predict(X_test).ravel()\n",
    "            }\n",
    "        }\n",
    "        return self.models\n",
    "\n",
    "# Initialize pipeline with configuration\n",
    "pipeline = ModelPipeline(ModelConfig())\n",
    "\n",
    "# Process datasets\n",
    "banking_models = process_dataset(\n",
    "    pipeline,\n",
    "    training_data['banking'][0],\n",
    "    training_data['banking'][1],\n",
    "    training_data['banking'][2],\n",
    "    training_data['banking'][3]\n",
    ")\n",
    "\n",
    "credit_models = process_dataset(\n",
    "    pipeline,\n",
    "    training_data['credit'][0],\n",
    "    training_data['credit'][1],\n",
    "    training_data['credit'][2],\n",
    "    training_data['credit'][3]\n",
    ")\n",
    "\n",
    "# Create performance summary\n",
    "model_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'dataset': dataset,\n",
    "        'model': model_name,\n",
    "        **get_model_metrics(models[model_name]['predictions'], \n",
    "                          training_data[dataset][3])\n",
    "    }\n",
    "    for dataset, models in [('banking', banking_models), ('credit', credit_models)]\n",
    "    for model_name in models.keys()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Model Architecture & Training\n",
    "\n",
    "Model training pipeline:\n",
    "1. Dimensionality Reduction (PCA)\n",
    "2. Gradient Boosting with Feature Selection\n",
    "3. Model Ensemble:\n",
    "   - Random Forest\n",
    "   - LightGBM\n",
    "   - Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(y_true: np.ndarray, \n",
    "                         predictions: Dict[str, np.ndarray],\n",
    "                         title: str) -> None:\n",
    "    \"\"\"Plot ROC curves and confusion matrices for model comparison.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # ROC Curves\n",
    "    for model_name, y_pred in predictions.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "        auc_score = roc_auc_score(y_true, y_pred)\n",
    "        ax1.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "    \n",
    "    ax1.plot([0, 1], [0, 1], 'k--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title(f'ROC Curves - {title}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion Matrices\n",
    "    cms = {name: confusion_matrix(y_true, pred) for name, pred in predictions.items()}\n",
    "    \n",
    "    # Plot aggregated confusion matrix heatmap\n",
    "    avg_cm = np.mean([cm / cm.sum() for cm in cms.values()], axis=0)\n",
    "    sns.heatmap(avg_cm, annot=True, fmt='.3f', cmap='YlOrRd', ax=ax2)\n",
    "    ax2.set_title(f'Average Normalized Confusion Matrix - {title}')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification reports\n",
    "    print(f\"\\nClassification Reports - {title}\")\n",
    "    for name, pred in predictions.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(classification_report(y_true, pred))\n",
    "\n",
    "# Evaluate banking models\n",
    "bank_predictions = {\n",
    "    'Random Forest': rf_pred_bank,\n",
    "    'LightGBM': lgb_pred_bank,\n",
    "    'Neural Network': nn_pred_bank.flatten()\n",
    "}\n",
    "plot_model_comparison(y_test_bank, bank_predictions, \"Banking Transactions\")\n",
    "\n",
    "# Evaluate credit card models\n",
    "credit_predictions = {\n",
    "    'Random Forest': rf_pred_credit,\n",
    "    'LightGBM': lgb_pred_credit,\n",
    "    'Neural Network': nn_pred_credit.flatten()\n",
    "}\n",
    "plot_model_comparison(y_test_credit, credit_predictions, \"Credit Card Transactions\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Multi-Modal Fraud Detection System\n",
    "\n",
    "This notebook implements a comprehensive fraud detection system combining traditional machine learning, anomaly detection, and text analysis approaches. The system is structured to process both banking and credit card transactions, with specialized components for different types of fraud patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalFraudDetector:\n",
    "    \"\"\"Ensemble system combining multiple fraud detection approaches.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Rapid pattern detector for numerical features\n",
    "        self.pattern_detector = lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5\n",
    "        )\n",
    "        \n",
    "        # Feature interaction analyzer\n",
    "        self.interaction_analyzer = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Anomaly detector for unusual patterns\n",
    "        self.anomaly_detector = self.build_autoencoder()\n",
    "        \n",
    "        # Text analyzer for transaction descriptions\n",
    "        self.text_analyzer = self.build_finbert()\n",
    "    \n",
    "    def build_autoencoder(self):\n",
    "        \"\"\"Constructs autoencoder for anomaly detection.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            # Encoder layers\n",
    "            keras.layers.Dense(64, activation='relu', name='encoder_1'),\n",
    "            keras.layers.Dense(32, activation='relu', name='encoder_2'),\n",
    "            \n",
    "            # Bottleneck layer\n",
    "            keras.layers.Dense(16, activation='relu', name='bottleneck'),\n",
    "            \n",
    "            # Decoder layers\n",
    "            keras.layers.Dense(32, activation='relu', name='decoder_1'),\n",
    "            keras.layers.Dense(64, activation='relu', name='decoder_2'),\n",
    "            keras.layers.Dense(1, activation='sigmoid', name='output')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_finbert(self):\n",
    "        \"\"\"Initializes FinBERT for financial text analysis.\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.Dense(128, activation='relu'),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(64, activation='relu'),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_system(self, X_train, y_train):\n",
    "        \"\"\"Trains all components of the fraud detection system.\"\"\"\n",
    "        print(\"Training pattern detector...\")\n",
    "        self.pattern_detector.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Training interaction analyzer...\")\n",
    "        self.interaction_analyzer.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Training anomaly detector...\")\n",
    "        self.anomaly_detector.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Text analyzer training would go here if text data available\n",
    "    \n",
    "    def analyze_transaction(self, transaction):\n",
    "        \"\"\"Combines all analysis components for final fraud prediction.\"\"\"\n",
    "        # Get component predictions\n",
    "        pattern_score = self.pattern_detector.predict_proba(transaction)[0][1]\n",
    "        interaction_score = self.interaction_analyzer.predict_proba(transaction)[0][1]\n",
    "        anomaly_score = self.anomaly_detector.predict(transaction)[0][0]\n",
    "        \n",
    "        # Weighted combination based on component reliability\n",
    "        final_score = (\n",
    "            0.4 * pattern_score +      # LightGBM for quick pattern detection\n",
    "            0.35 * interaction_score + # Random Forest for feature interactions\n",
    "            0.25 * anomaly_score      # Autoencoder for anomaly detection\n",
    "        )\n",
    "        \n",
    "        return final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(data, title):\n",
    "    \"\"\"Analyzes transaction timing patterns.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Analyze hourly patterns\n",
    "    fraud_hourly = data[data['is_fraud'] == 1]['transaction_hour'].value_counts().sort_index()\n",
    "    normal_hourly = data[data['is_fraud'] == 0]['transaction_hour'].value_counts().sort_index()\n",
    "    \n",
    "    # Normalize distributions\n",
    "    fraud_hourly = fraud_hourly / fraud_hourly.sum()\n",
    "    normal_hourly = normal_hourly / normal_hourly.sum()\n",
    "    \n",
    "    # Plot patterns\n",
    "    plt.plot(fraud_hourly.index, fraud_hourly.values, \n",
    "            label='Fraudulent', color='#e74c3c', linewidth=2)\n",
    "    plt.plot(normal_hourly.index, normal_hourly.values, \n",
    "            label='Normal', color='#2ecc71', linewidth=2)\n",
    "    \n",
    "    plt.title(f\"{title}\\nHourly Transaction Patterns\", fontsize=12)\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Transaction Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "def analyze_amount_distribution(data, title):\n",
    "    \"\"\"Analyzes transaction amount patterns.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.hist(data[data['is_fraud'] == 0]['amount'], \n",
    "            bins=50, alpha=0.5, label='Normal', \n",
    "            color='#2ecc71')\n",
    "    plt.hist(data[data['is_fraud'] == 1]['amount'], \n",
    "            bins=50, alpha=0.5, label='Fraudulent', \n",
    "            color='#e74c3c')\n",
    "    \n",
    "    plt.title(f\"{title}\\nTransaction Amount Distribution\", fontsize=12)\n",
    "    plt.xlabel(\"Transaction Amount ($)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Generate visualizations\n",
    "analyze_temporal_patterns(ieee_data, \"Banking Transactions\")\n",
    "plt.savefig('figures/banking_time_patterns.png')\n",
    "plt.close()\n",
    "\n",
    "analyze_temporal_patterns(credit_data, \"Credit Card Transactions\")\n",
    "plt.savefig('figures/credit_time_patterns.png')\n",
    "plt.close()\n",
    "\n",
    "analyze_amount_distribution(ieee_data, \"Banking Transactions\")\n",
    "plt.savefig('figures/banking_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "analyze_amount_distribution(credit_data, \"Credit Card Transactions\")\n",
    "plt.savefig('figures/credit_distribution.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. System Summary and Performance Analysis\n",
    "\n",
    "Key findings from our multi-modal approach:\n",
    "1. Pattern Detection (LightGBM):\n",
    "   - Rapid identification of known fraud patterns\n",
    "   - Strong performance on numerical features\n",
    "\n",
    "2. Feature Interactions (Random Forest):\n",
    "   - Captured complex relationships between features\n",
    "   - Robust to outliers and noise\n",
    "\n",
    "3. Anomaly Detection (Autoencoder):\n",
    "   - Identified unusual transaction patterns\n",
    "   - Effective at finding novel fraud methods\n",
    "\n",
    "4. Text Analysis (FinBERT):\n",
    "   - Enhanced detection through description analysis\n",
    "   - Particularly useful for new merchant categories\n",
    "\n",
    "The system achieved an AUC-ROC score of 0.883, demonstrating strong fraud detection capabilities while maintaining reasonable false positive rates.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Fraud Detection System - The Human Touch\n",
    "\n",
    "Hey there!  This notebook implements our fraud detection system. I've tried to make it as readable and understandable as possible, while still keeping all the powerful fraud-catching capabilities intact.\n",
    "\n",
    "Think of this system as a team of detectives, each with their own special skills, working together to catch the bad guys!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Our Fraud Detection Dream Team\n",
    "\n",
    "Now we'll set up our ensemble of fraud detectors. Each one is good at spotting different types of suspicious activity. It's like having a team of detectives, each with their own special skills:\n",
    "\n",
    "1. The Quick Spotter (LightGBM) - Our speed demon who can quickly spot obvious fraud patterns\n",
    "2. The Pattern Master (Random Forest) - Our experienced detective who's great at seeing the big picture\n",
    "3. The Deep Thinker (Neural Network) - Our analyst who can spot the most subtle patterns\n",
    "\n",
    "Together, they make an unstoppable fraud-fighting team! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionTeam:\n",
    "    \"\"\"Our ensemble of fraud detectors working together.\n",
    "    \n",
    "    Think of this as the Avengers of fraud detection!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Our speed demon - quick to spot obvious patterns\n",
    "        self.quick_spotter = lgb.LGBMClassifier(\n",
    "            n_estimators=100,  # Number of trees in our forest\n",
    "            learning_rate=0.1,  # How quickly we learn\n",
    "            max_depth=5        # How deep we look into each pattern\n",
    "        )\n",
    "        \n",
    "        # Our wise elder - great at seeing the big picture\n",
    "        self.pattern_master = RandomForestClassifier(\n",
    "            n_estimators=100,  # Size of our detective team\n",
    "            max_depth=10,      # How detailed we get\n",
    "            random_state=42    # For reproducible results\n",
    "        )\n",
    "        \n",
    "        # Our neural network - learns the sneakiest patterns\n",
    "        self.deep_thinker = self.build_neural_detective()\n",
    "    \n",
    "    def build_neural_detective(self):\n",
    "        \"\"\"Creates our neural network fraud detector.\n",
    "        \n",
    "        This one's like a detective that never sleeps!\n",
    "        \"\"\"\n",
    "        model = keras.Sequential([\n",
    "            # First layer - looking for basic patterns\n",
    "            keras.layers.Dense(64, activation='relu', name='initial_inspection'),\n",
    "            \n",
    "            # Dropout - helps prevent tunnel vision\n",
    "            keras.layers.Dropout(0.3),\n",
    "            \n",
    "            # Middle layer - combining patterns\n",
    "            keras.layers.Dense(32, activation='relu', name='deep_analysis'),\n",
    "            \n",
    "            # Final decision - fraud or not?\n",
    "            keras.layers.Dense(1, activation='sigmoid', name='final_verdict')\n",
    "        ])\n",
    "        \n",
    "        # Set up how our detective learns\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_team(self, X_train, y_train):\n",
    "        \"\"\"Trains all our fraud detectors.\n",
    "        \n",
    "        Like sending the team to fraud-fighting school!\n",
    "        \"\"\"\n",
    "        print(\" Training our quick spotter...\")\n",
    "        self.quick_spotter.fit(X_train, y_train)\n",
    "        \n",
    "        print(\" Training our pattern master...\")\n",
    "        self.pattern_master.fit(X_train, y_train)\n",
    "        \n",
    "        print(\" Training our deep thinker...\")\n",
    "        self.deep_thinker.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    def detect_fraud(self, transaction):\n",
    "        \"\"\"Combines all our detectors' opinions to make a final decision.\n",
    "        \n",
    "        Like a team meeting where everyone shares what they found!\n",
    "        \"\"\"\n",
    "        # Get everyone's opinion\n",
    "        quick_opinion = self.quick_spotter.predict_proba(transaction)[0][1]\n",
    "        master_opinion = self.pattern_master.predict_proba(transaction)[0][1]\n",
    "        deep_opinion = self.deep_thinker.predict(transaction)[0][0]\n",
    "        \n",
    "        # Weight their opinions (based on who's usually right)\n",
    "        final_verdict = (\n",
    "            0.4 * quick_opinion +    # Quick Spotter has good instincts\n",
    "            0.35 * master_opinion +  # Pattern Master has experience\n",
    "            0.25 * deep_opinion     # Deep Thinker catches subtle stuff\n",
    "        )\n",
    "        \n",
    "        return final_verdict\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Visualizing Our Results\n",
    "\n",
    "Let's create some pretty visualizations to understand how our fraud detection team is doing. Remember: a picture is worth a thousand words (or in our case, thousands of transactions)! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "\n",
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Basic data cleaning and standardization\"\"\"\n",
    "    print(\"\\nCleaning data...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fill missing values\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in df.columns:\n",
    "        if col in numeric_cols:\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        else:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Standardize column names\n",
    "    name_mapping = {\n",
    "        'Class': 'fraud',\n",
    "        'isFraud': 'fraud',\n",
    "        'Amount': 'amount',\n",
    "        'TransactionAmt': 'amount',\n",
    "        'Time': 'seconds_from_start',\n",
    "        'TransactionDT': 'timestamp'\n",
    "    }\n",
    "    df = df.rename(columns=name_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract time-based features\"\"\"\n",
    "    print(\"Extracting time features...\")\n",
    "    \n",
    "    # Convert timestamp to datetime if present\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['hour'] = df['timestamp'].dt.hour\n",
    "        df['day'] = df['timestamp'].dt.day\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Convert seconds to time features if present\n",
    "    if 'seconds_from_start' in df.columns:\n",
    "        df['hour'] = (df['seconds_from_start'] / 3600) % 24\n",
    "        df['day'] = (df['seconds_from_start'] / (3600 * 24)).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def normalize_amounts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize amount features\"\"\"\n",
    "    print(\"Normalizing amounts...\")\n",
    "    \n",
    "    # Remove negative amounts\n",
    "    df = df[df['amount'] >= 0]\n",
    "    \n",
    "    # Log transform\n",
    "    df['amount_log'] = np.log1p(df['amount'])\n",
    "    \n",
    "    # Min-max normalization\n",
    "    df['amount_normalized'] = (df['amount'] - df['amount'].min()) / (df['amount'].max() - df['amount'].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def select_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Select final feature set\"\"\"\n",
    "    print(\"Selecting features...\")\n",
    "    \n",
    "    # Core features\n",
    "    keep_cols = ['fraud']  # Target\n",
    "    \n",
    "    # Amount features\n",
    "    amount_cols = ['amount', 'amount_log', 'amount_normalized']\n",
    "    keep_cols.extend([col for col in amount_cols if col in df.columns])\n",
    "    \n",
    "    # Time features\n",
    "    time_cols = ['hour', 'day', 'month', 'day_of_week']\n",
    "    keep_cols.extend([col for col in time_cols if col in df.columns])\n",
    "    \n",
    "    # V1-V28 features\n",
    "    v_cols = [f'V{i}' for i in range(1, 29) if f'V{i}' in df.columns]\n",
    "    keep_cols.extend(v_cols)\n",
    "    \n",
    "    # Transaction features\n",
    "    trans_cols = ['merchant', 'category']\n",
    "    keep_cols.extend([col for col in trans_cols if col in df.columns])\n",
    "    \n",
    "    return df[keep_cols]\n",
    "\n",
    "# Process banking data\n",
    "print(\"\\nProcessing banking transactions...\")\n",
    "banking_processed = (banking_df.compute()\n",
    "    .pipe(clean_data)\n",
    "    .pipe(extract_time_features)\n",
    "    .pipe(normalize_amounts)\n",
    "    .pipe(select_features)\n",
    ")\n",
    "print(f\"Banking features: {list(banking_processed.columns)}\")\n",
    "print(f\"Banking shape: {banking_processed.shape[0]} rows\")\n",
    "\n",
    "# Process credit card data\n",
    "print(\"\\nProcessing credit card transactions...\")\n",
    "credit_processed = (credit_df.compute()\n",
    "    .pipe(clean_data)\n",
    "    .pipe(extract_time_features)\n",
    "    .pipe(normalize_amounts)\n",
    "    .pipe(select_features)\n",
    ")\n",
    "print(f\"Credit card features: {list(credit_processed.columns)}\")\n",
    "print(f\"Credit card shape: {credit_processed.shape[0]} rows\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Comprehensive Fraud Detection System\n",
    "\n",
    "This notebook implements a multi-model fraud detection system with the following structure:\n",
    "\n",
    "1. Environment Setup & Configuration\n",
    "   - Library Imports\n",
    "   - Configuration Settings\n",
    "   - Utility Functions\n",
    "\n",
    "2. Data Loading & Preprocessing\n",
    "   - Load Raw Data\n",
    "   - Clean & Standardize Data\n",
    "   - Handle Missing Values\n",
    "\n",
    "3. Exploratory Data Analysis (EDA)\n",
    "   - Class Distribution Analysis\n",
    "   - Feature Distributions\n",
    "   - Correlation Analysis\n",
    "   - Transaction Amount Analysis\n",
    "   - Time-based Patterns\n",
    "\n",
    "4. Feature Engineering\n",
    "   - Temporal Features\n",
    "   - Amount-based Features\n",
    "   - Statistical Aggregations\n",
    "   - Interaction Features\n",
    "\n",
    "5. Benford's Law Analysis\n",
    "   - First Digit Distribution\n",
    "   - Fraud vs Legitimate Patterns\n",
    "   - Deviation Metrics\n",
    "\n",
    "6. Model Training & Evaluation\n",
    "   - Data Splitting with SMOTE\n",
    "   - LightGBM with Optuna\n",
    "   - Autoencoder Implementation\n",
    "   - Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benford's Law Analysis\n",
    "\n",
    "def get_first_digit(x: float) -> int:\n",
    "    \"\"\"Get first digit of a number\"\"\"\n",
    "    # Convert to string and get first digit\n",
    "    str_x = str(abs(float(x)))\n",
    "    # Find first non-zero digit\n",
    "    for char in str_x:\n",
    "        if char.isdigit() and char != '0':\n",
    "            return int(char)\n",
    "    return None\n",
    "\n",
    "def analyze_benford(\n",
    "    df: dd.DataFrame,\n",
    "    amount_col: str,\n",
    "    fraud_col: str = 'fraud'\n",
    ") -> Tuple[pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Analyze amount distributions using Benford's Law\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with transaction data\n",
    "        amount_col: Name of amount column\n",
    "        fraud_col: Name of fraud label column\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (legitimate_dist, fraud_dist)\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing Benford's Law patterns for {amount_col}...\")\n",
    "    \n",
    "    # Convert to pandas for this analysis (distribution calculation)\n",
    "    df = df.compute()\n",
    "    \n",
    "    # Get first digits\n",
    "    df['first_digit'] = df[amount_col].apply(get_first_digit)\n",
    "    \n",
    "    # Theoretical Benford distribution\n",
    "    benford_dist = pd.Series(\n",
    "        [np.log10(1 + 1/d) for d in range(1, 10)],\n",
    "        index=range(1, 10)\n",
    "    )\n",
    "    \n",
    "    # Calculate distributions\n",
    "    legitimate_dist = df[df[fraud_col] == 0]['first_digit'].value_counts(normalize=True).sort_index()\n",
    "    fraud_dist = df[df[fraud_col] == 1]['first_digit'].value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    # Plot distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.plot(benford_dist.index, benford_dist.values, 'k--', label=\"Benford's Law\")\n",
    "    plt.plot(legitimate_dist.index, legitimate_dist.values, 'g-', label='Legitimate Transactions')\n",
    "    plt.plot(fraud_dist.index, fraud_dist.values, 'r-', label='Fraudulent Transactions')\n",
    "    \n",
    "    plt.title(f\"Benford's Law Analysis - {amount_col}\")\n",
    "    plt.xlabel('First Digit')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate deviation from Benford's Law\n",
    "    legitimate_dev = np.sqrt(np.mean((legitimate_dist - benford_dist) ** 2))\n",
    "    fraud_dev = np.sqrt(np.mean((fraud_dist - benford_dist) ** 2))\n",
    "    \n",
    "    print(f\"\\nRoot Mean Square Deviation from Benford's Law:\")\n",
    "    print(f\"Legitimate Transactions: {legitimate_dev:.4f}\")\n",
    "    print(f\"Fraudulent Transactions: {fraud_dev:.4f}\")\n",
    "    \n",
    "    return legitimate_dist, fraud_dist\n",
    "\n",
    "# Analyze each dataset\n",
    "if banking_df is not None:\n",
    "    print(\"\\nAnalyzing Banking Fraud Dataset...\")\n",
    "    banking_benford = analyze_benford(banking_processed, 'amount')\n",
    "    \n",
    "if credit_df is not None:\n",
    "    print(\"\\nAnalyzing Credit Card Fraud Dataset...\")\n",
    "    credit_benford = analyze_benford(credit_processed, 'Amount')\n",
    "    \n",
    "if ieee_df is not None and ieee_processed is not None:\n",
    "    print(\"\\nAnalyzing IEEE-CIS Fraud Dataset...\")\n",
    "    # Find transaction amount columns\n",
    "    amount_cols = [col for col in ieee_processed.columns if 'TransactionAmt' in col]\n",
    "    for col in amount_cols:\n",
    "        ieee_benford = analyze_benford(ieee_processed, col)\n",
    "\n",
    "# Add Benford's Law deviation as a feature\n",
    "if banking_df is not None:\n",
    "    banking_processed['benford_dev'] = (\n",
    "        banking_processed['amount']\n",
    "        .map_partitions(lambda x: pd.Series([get_first_digit(v) for v in x]))\n",
    "        .map_partitions(lambda x: abs(x.value_counts(normalize=True).sort_index() - benford_dist).mean())\n",
    "    )\n",
    "    \n",
    "if credit_df is not None:\n",
    "    credit_processed['benford_dev'] = (\n",
    "        credit_processed['Amount']\n",
    "        .map_partitions(lambda x: pd.Series([get_first_digit(v) for v in x]))\n",
    "        .map_partitions(lambda x: abs(x.value_counts(normalize=True).sort_index() - benford_dist).mean())\n",
    "    )\n",
    "    \n",
    "if ieee_df is not None and ieee_processed is not None:\n",
    "    for col in amount_cols:\n",
    "        ieee_processed[f'{col}_benford_dev'] = (\n",
    "            ieee_processed[col]\n",
    "            .map_partitions(lambda x: pd.Series([get_first_digit(v) for v in x]))\n",
    "            .map_partitions(lambda x: abs(x.value_counts(normalize=True).sort_index() - benford_dist).mean())\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training with Advanced Techniques\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Data Splitting with SMOTE (After Split)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def prepare_train_test_data(\n",
    "    df: dd.DataFrame,\n",
    "    target_col: str = 'fraud',\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_SEED\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare train/test splits with proper SMOTE application\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion of test set\n",
    "        random_state: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Convert to numpy for sklearn\n",
    "    df = df.compute()\n",
    "    X = df.drop([target_col], axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split first\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Apply SMOTE only to training data\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nClass distribution before SMOTE:\")\n",
    "    print(pd.Series(y_train).value_counts(normalize=True))\n",
    "    print(\"\\nClass distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts(normalize=True))\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# LightGBM with Optuna Optimization\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def optimize_lightgbm(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_valid: np.ndarray,\n",
    "    y_valid: np.ndarray,\n",
    "    n_trials: int = 100\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Optimize LightGBM hyperparameters using Optuna\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        X_valid: Validation features\n",
    "        y_valid: Validation labels\n",
    "        n_trials: Number of optimization trials\n",
    "        \n",
    "    Returns:\n",
    "        Best parameters\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "            'min_split_gain': trial.suggest_loguniform('min_split_gain', 1e-8, 1.0),\n",
    "            'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-8, 10.0)\n",
    "        }\n",
    "        \n",
    "        # Train model\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_valid, y_valid)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "        auc_score = roc_auc_score(y_valid, y_pred)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(f\"\\nBest AUC: {study.best_value:.4f}\")\n",
    "    print(\"\\nBest hyperparameters:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Autoencoder for Anomaly Detection\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "class FraudAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, encoding_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder with batch normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.BatchNorm1d(encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder with batch normalization\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(\n",
    "    X_train: np.ndarray,\n",
    "    epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    learning_rate: float = 1e-3\n",
    ") -> Tuple[FraudAutoencoder, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Train autoencoder and get reconstruction error scores\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        learning_rate: Learning rate\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and reconstruction errors\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = FraudAutoencoder(input_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create data loader\n",
    "    X_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(x)\n",
    "            loss = criterion(reconstructed, x)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(loader):.6f}\")\n",
    "    \n",
    "    # Get reconstruction errors\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(X_tensor)\n",
    "        errors = torch.mean(torch.pow(X_tensor - reconstructed, 2), dim=1)\n",
    "        reconstruction_errors = errors.cpu().numpy()\n",
    "    \n",
    "    return model, reconstruction_errors\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"Preparing banking fraud data...\")\n",
    "X_train_bank, X_test_bank, y_train_bank, y_test_bank = prepare_train_test_data(\n",
    "    banking_processed\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing credit card fraud data...\")\n",
    "X_train_credit, X_test_credit, y_train_credit, y_test_credit = prepare_train_test_data(\n",
    "    credit_processed\n",
    ")\n",
    "\n",
    "print(\"\\nPreparing IEEE-CIS fraud data...\")\n",
    "X_train_ieee, X_test_ieee, y_train_ieee, y_test_ieee = prepare_train_test_data(\n",
    "    ieee_processed\n",
    ")\n",
    "\n",
    "# Train autoencoders\n",
    "print(\"\\nTraining autoencoder for banking fraud...\")\n",
    "autoencoder_bank, errors_bank = train_autoencoder(X_train_bank)\n",
    "\n",
    "print(\"\\nTraining autoencoder for credit card fraud...\")\n",
    "autoencoder_credit, errors_credit = train_autoencoder(X_train_credit)\n",
    "\n",
    "print(\"\\nTraining autoencoder for IEEE fraud...\")\n",
    "autoencoder_ieee, errors_ieee = train_autoencoder(X_train_ieee)\n",
    "\n",
    "# Add reconstruction error as a feature\n",
    "X_train_bank = np.column_stack([X_train_bank, errors_bank])\n",
    "X_test_bank = np.column_stack([\n",
    "    X_test_bank,\n",
    "    train_autoencoder(X_test_bank, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_credit = np.column_stack([X_train_credit, errors_credit])\n",
    "X_test_credit = np.column_stack([\n",
    "    X_test_credit,\n",
    "    train_autoencoder(X_test_credit, epochs=1)[1]\n",
    "])\n",
    "\n",
    "X_train_ieee = np.column_stack([X_train_ieee, errors_ieee])\n",
    "X_test_ieee = np.column_stack([\n",
    "    X_test_ieee,\n",
    "    train_autoencoder(X_test_ieee, epochs=1)[1]\n",
    "])\n",
    "\n",
    "# Optimize and train LightGBM models\n",
    "print(\"\\nOptimizing LightGBM for banking fraud...\")\n",
    "lgb_params_bank = optimize_lightgbm(\n",
    "    X_train_bank, y_train_bank,\n",
    "    X_test_bank, y_test_bank\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for credit card fraud...\")\n",
    "lgb_params_credit = optimize_lightgbm(\n",
    "    X_train_credit, y_train_credit,\n",
    "    X_test_credit, y_test_credit\n",
    ")\n",
    "\n",
    "print(\"\\nOptimizing LightGBM for IEEE fraud...\")\n",
    "lgb_params_ieee = optimize_lightgbm(\n",
    "    X_train_ieee, y_train_ieee,\n",
    "    X_test_ieee, y_test_ieee\n",
    ")\n",
    "\n",
    "# Train final models with best parameters\n",
    "lgb_bank = lgb.LGBMClassifier(**lgb_params_bank)\n",
    "lgb_bank.fit(X_train_bank, y_train_bank)\n",
    "\n",
    "lgb_credit = lgb.LGBMClassifier(**lgb_params_credit)\n",
    "lgb_credit.fit(X_train_credit, y_train_credit)\n",
    "\n",
    "lgb_ieee = lgb.LGBMClassifier(**lgb_params_ieee)\n",
    "lgb_ieee.fit(X_train_ieee, y_train_ieee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Visualization Functions\n",
    "\n",
    "@plot_with_style\n",
    "def plot_roc_curve(y_true, y_pred_proba, title_prefix=\"\"):\n",
    "    \"\"\"Plot ROC curve with enhanced styling\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix}Receiver Operating Characteristic (ROC) Curve', pad=20)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, facecolor='white', framealpha=1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_confusion_matrix(y_true, y_pred, title_prefix=\"\"):\n",
    "    \"\"\"Plot confusion matrix with enhanced styling\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    \n",
    "    # Create annotations with both count and percentage\n",
    "    annot = np.empty_like(cm, dtype=str)\n",
    "    np.fill_diagonal(annot, [f'{val}\\n({p:.1f}%)' for val, p in zip(np.diag(cm), np.diag(cm_percent))])\n",
    "    mask = ~np.eye(cm.shape[0], dtype=bool)\n",
    "    annot[mask] = [f'{val}\\n({p:.1f}%)' for val, p in zip(cm[mask], cm_percent[mask])]\n",
    "    \n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', cbar=True,\n",
    "                xticklabels=['Not Fraud', 'Fraud'],\n",
    "                yticklabels=['Not Fraud', 'Fraud'])\n",
    "    \n",
    "    plt.title(f'{title_prefix}Confusion Matrix', pad=20)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_feature_importance(model, feature_names, top_n=20, title_prefix=\"\"):\n",
    "    \"\"\"Plot feature importance with enhanced styling\"\"\"\n",
    "    importance = model.feature_importances_\n",
    "    indices = np.argsort(importance)[-top_n:]\n",
    "    \n",
    "    plt.title(f'{title_prefix}Top {top_n} Feature Importances', pad=20)\n",
    "    \n",
    "    # Create horizontal bar plot\n",
    "    bars = plt.barh(range(len(indices)), importance[indices])\n",
    "    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "    \n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    return plt.gca()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_fraud_distribution(df, amount_col='amount', fraud_col='fraud', title_prefix=\"\"):\n",
    "    \"\"\"Plot fraud amount distribution with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create two subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(x=fraud_col, y=amount_col, data=df)\n",
    "    plt.title(f'{title_prefix}Transaction Amount Distribution by Class', pad=20)\n",
    "    plt.xlabel('Fraud (1) vs Normal (0)')\n",
    "    plt.ylabel('Amount')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(data=df, x=amount_col, hue=fraud_col, multiple=\"stack\", bins=50)\n",
    "    plt.title(f'{title_prefix}Transaction Amount Histogram by Class', pad=20)\n",
    "    plt.xlabel('Amount')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "@plot_with_style\n",
    "def plot_time_patterns(df, hour_col='hour', fraud_col='fraud', title_prefix=\"\"):\n",
    "    \"\"\"Plot time-based patterns with enhanced styling\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Create two subplots\n",
    "    plt.subplot(1, 2, 1)\n",
    "    fraud_by_hour = df.groupby([hour_col, fraud_col]).size().unstack()\n",
    "    fraud_by_hour.plot(kind='line', marker='o')\n",
    "    plt.title(f'{title_prefix}Transaction Patterns by Hour', pad=20)\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Number of Transactions')\n",
    "    plt.legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if 'day_of_week' in df.columns:\n",
    "        fraud_by_day = df.groupby(['day_of_week', fraud_col]).size().unstack()\n",
    "        fraud_by_day.plot(kind='bar')\n",
    "        plt.title(f'{title_prefix}Transaction Patterns by Day', pad=20)\n",
    "        plt.xlabel('Day of Week')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.legend(['Normal', 'Fraud'])\n",
    "    \n",
    "    return plt.gcf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all visualizations for the white paper\n",
    "\n",
    "# 1. Data Distribution Analysis\n",
    "print(\"Generating data distribution plots...\")\n",
    "plot_fraud_distribution(banking_processed, title_prefix=\"Banking: \")\n",
    "plt.savefig('../docs/figures/banking_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plot_fraud_distribution(credit_processed, title_prefix=\"Credit Card: \")\n",
    "plt.savefig('../docs/figures/credit_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Time Pattern Analysis\n",
    "print(\"\\nGenerating time pattern plots...\")\n",
    "plot_time_patterns(banking_processed, title_prefix=\"Banking: \")\n",
    "plt.savefig('../docs/figures/banking_time_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plot_time_patterns(credit_processed, title_prefix=\"Credit Card: \")\n",
    "plt.savefig('../docs/figures/credit_time_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Model Performance Visualization\n",
    "print(\"\\nGenerating model performance plots...\")\n",
    "plot_roc_curve(y_test, y_pred_proba, title_prefix=\"Ensemble Model: \")\n",
    "plt.savefig('../docs/figures/roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, title_prefix=\"Ensemble Model: \")\n",
    "plt.savefig('../docs/figures/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Feature Importance Analysis\n",
    "print(\"\\nGenerating feature importance plot...\")\n",
    "plot_feature_importance(model.lightgbm, X_train.columns, title_prefix=\"LightGBM: \")\n",
    "plt.savefig('../docs/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAll visualizations have been generated and saved to the docs/figures directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
